{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 (a) (b) (c) Gaussian Distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For independent random variables $X_1 = X$ and $X_2 = Y$, the distribution $f_Z$ of $Z = X_3 = X + Y$ equals the convolution of $X$  and $Y$:\n",
    "   \n",
    "<math>\n",
    "    \\begin{align}\n",
    "    f_Z(z) = \\int_{-\\infty}^\\infty f_Y(z-x) f_X(x) \\, dx\n",
    "    \\end{align}\n",
    "</math>\n",
    "\n",
    "Given that $f_X$ and $f_Y$ are normal densities:\n",
    "\n",
    "<math>\n",
    "\\begin{align}\n",
    "f_X(x) = \\mathcal{N}(x| \\mu_X, \\sigma_X^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma_X} e^{-(x-\\mu_X)^2/(2\\sigma_X^2)} \\\\[1pt]\n",
    "f_Y(y) = \\mathcal{N}(y| \\mu_Y, \\sigma_Y^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma_Y} e^{-(y-\\mu_Y)^2/(2\\sigma_Y^2)}\n",
    "\\end{align}\n",
    "</math>\n",
    "\n",
    "Substituting into the convolution:\n",
    "\n",
    "<math>\n",
    "\\begin{align}\n",
    "f_Z(z)\n",
    "&= \\int_{-\\infty}^\\infty\n",
    "   \\frac{1}{\\sqrt{2\\pi}\\sigma_Y}\n",
    "   \\exp \\left[-{(z-x-\\mu_Y)^2 \\over 2\\sigma_Y^2}\\right]\n",
    "   \\frac{1}{\\sqrt{2\\pi}\\sigma_X}\n",
    "   \\exp \\left[-{(x-\\mu_X)^2 \\over 2\\sigma_X^2}\\right]\n",
    "   \\, dx \n",
    "\\\\[1pt]\n",
    "&= \\int_{-\\infty}^\\infty\n",
    "   \\frac{1}{\\sqrt{2\\pi}\\sqrt{2\\pi}\\sigma_X\\sigma_Y}\n",
    "   \\exp \\left[-\\frac{\\sigma_X^2(z-x-\\mu_Y)^2 + \\sigma_Y^2(x-\\mu_X)^2}{2\\sigma_X^2\\sigma_Y^2}\\right]\n",
    "   \\, dx \n",
    "\\\\[1pt]\n",
    "&= \\int_{-\\infty}^\\infty\n",
    "   \\frac{1}{\\sqrt{2\\pi}\\sqrt{2\\pi}\\sigma_X\\sigma_Y}\n",
    "   \\exp \\left[\n",
    "      -\\frac\n",
    "         {\\sigma_X^2(z^2 + x^2 + \\mu_Y^2 - 2xz - 2z\\mu_Y + 2x\\mu_Y) + \\sigma_Y^2(x^2 + \\mu_X^2 - 2x\\mu_X)}\n",
    "         {2\\sigma_Y^2\\sigma_X^2}\n",
    "   \\right]\n",
    "   \\, dx \n",
    "\\\\[1pt]\n",
    "&= \\int_{-\\infty}^\\infty\n",
    "   \\frac{1}{\\sqrt{2\\pi}\\sqrt{2\\pi}\\sigma_X\\sigma_Y}\n",
    "   \\exp \\left[\n",
    "      -\\frac\n",
    "         {\n",
    "            x^2(\\sigma_X^2 + \\sigma_Y^2) - \n",
    "            2x(\\sigma_X^2(z - \\mu_Y) + \\sigma_Y^2\\mu_X) +\n",
    "            \\sigma_X^2(z^2 + \\mu_Y^2 - 2z\\mu_Y) + \\sigma_Y^2\\mu_X^2\n",
    "         }\n",
    "         {2\\sigma_Y^2\\sigma_X^2}\n",
    "   \\right]\n",
    "   \\, dx\n",
    "\\\\[1pt]\n",
    "\\end{align}\n",
    "</math>\n",
    "\n",
    "Defining <math>$\\sigma_Z = \\sqrt{\\sigma_X^2 + \\sigma_Y^2}$</math>:\n",
    "\n",
    "<math>\n",
    "\\begin{align}\n",
    "f_Z(z)\n",
    "&= \\int_{-\\infty}^\\infty\n",
    "   \\frac{1}{\\sqrt{2\\pi}\\sigma_Z}\n",
    "   \\frac{1}{\\sqrt{2\\pi}\\frac{\\sigma_X\\sigma_Y}{\\sigma_Z}}\n",
    "   \\exp \\left[\n",
    "      -\\frac\n",
    "         {\n",
    "            x^2 - \n",
    "            2x\\frac{\\sigma_X^2(z - \\mu_Y) + \\sigma_Y^2\\mu_X}{\\sigma_Z^2} +\n",
    "            \\frac{\\sigma_X^2(z^2 + \\mu_Y^2 - 2z\\mu_Y) + \\sigma_Y^2\\mu_X^2}{\\sigma_Z^2}\n",
    "         }\n",
    "         {2\\left(\\frac{\\sigma_X\\sigma_Y}{\\sigma_Z}\\right)^2}\n",
    "   \\right]\n",
    "   \\, dx \n",
    "\\\\[1pt]\n",
    "&= \\int_{-\\infty}^\\infty\n",
    "   \\frac{1}{\\sqrt{2\\pi}\\sigma_Z}\n",
    "   \\frac{1}{\\sqrt{2\\pi}\\frac{\\sigma_X\\sigma_Y}{\\sigma_Z}}\n",
    "   \\exp \\left[\n",
    "      -\\frac\n",
    "         {\n",
    "            \\left(x - \\frac{\\sigma_X^2(z - \\mu_Y) + \\sigma_Y^2\\mu_X}{\\sigma_Z^2}\\right)^2 -\n",
    "            \\left(\\frac{\\sigma_X^2(z - \\mu_Y) + \\sigma_Y^2\\mu_X}{\\sigma_Z^2}\\right)^2 +\n",
    "            \\frac{\\sigma_X^2(z - \\mu_Y)^2 + \\sigma_Y^2\\mu_X^2}{\\sigma_Z^2}\n",
    "         }\n",
    "         {2\\left(\\frac{\\sigma_X\\sigma_Y}{\\sigma_Z}\\right)^2}\n",
    "   \\right]\n",
    "   \\, dx \n",
    "\\\\[1pt]\n",
    "&= \\int_{-\\infty}^\\infty\n",
    "   \\frac{1}{\\sqrt{2\\pi}\\sigma_Z}\n",
    "   \\exp \\left[\n",
    "      -\\frac\n",
    "         {\n",
    "            \\sigma_Z^2\\left(\\sigma_X^2(z - \\mu_Y)^2 + \\sigma_Y^2\\mu_X^2\\right) -\n",
    "            \\left(\\sigma_X^2(z - \\mu_Y) + \\sigma_Y^2\\mu_X\\right)^2\n",
    "         }\n",
    "         {2\\sigma_Z^2\\left(\\sigma_X\\sigma_Y\\right)^2}\n",
    "   \\right]\n",
    "   \\frac{1}{\\sqrt{2\\pi}\\frac{\\sigma_X\\sigma_Y}{\\sigma_Z}}\n",
    "   \\exp \\left[\n",
    "      -\\frac\n",
    "         {\n",
    "            \\left(x - \\frac{\\sigma_X^2(z - \\mu_Y) + \\sigma_Y^2\\mu_X}{\\sigma_Z^2}\\right)^2\n",
    "         }\n",
    "         {2\\left(\\frac{\\sigma_X\\sigma_Y}{\\sigma_Z}\\right)^2}\n",
    "   \\right]\n",
    "   \\, dx \n",
    "\\\\[1pt]\n",
    "&= \\frac{1}{\\sqrt{2\\pi}\\sigma_Z}\n",
    "   \\exp \\left[ - { (z-(\\mu_X+\\mu_Y))^2 \\over 2\\sigma_Z^2 } \\right]\n",
    "   \\int_{-\\infty}^{\\infty}\n",
    "   \\frac{1}{\\sqrt{2\\pi}\\frac{\\sigma_X\\sigma_Y}{\\sigma_Z}}\n",
    "   \\exp \\left[ - \\frac{\\left(x-\\frac{\\sigma_X^2(z-\\mu_Y)+\\sigma_Y^2\\mu_X}{\\sigma_Z^2}\\right)^2}{2\\left(\\frac{\\sigma_X\\sigma_Y}{\\sigma_Z}\\right)^2} \\right]\n",
    "   \\, dx\n",
    "\\end{align}\n",
    "</math>\n",
    "\n",
    "The expression in the integral is a normal density distribution on $X$, and so the integral equals 1. Thus:\n",
    "\n",
    "<math> $f_Z(z) = \\frac{1}{\\sqrt{2\\pi}\\sigma_Z} \\exp \\left[ - { (z-(\\mu_X+\\mu_Y))^2 \\over 2\\sigma_Z^2 } \\right] $</math>   \n",
    "   \n",
    "   \n",
    "So, $f_Z(z) = \\mathcal{N}(z | \\mu_Z, \\sigma_Z^2) = \\mathcal{N}(z | \\mu_X + \\mu_Y, \\sigma_X^2 + \\sigma_Y^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 (d) Multivariate Gaussian Distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fact**: The sum of independent Gaussian random variables is Gaussian.   \n",
    "*Proof*  \n",
    "Given independent multivariate Gaussian random variables $\\textbf Y, \\textbf Z$.   \n",
    "We define  \n",
    "\n",
    "$$ \\textbf X = \\textbf Y + \\textbf Z$$\n",
    "Then, it can be written as a linear combination: \n",
    "\n",
    "$$\\textbf L \\cdot \\textbf X = \\sum_{i=1}^{N}L_iX_i = \\sum_{i=1}^{N}(L_iY_i + L_iZ_i) = \n",
    "\\sum_{i=1}^{N}L_iY_i + \\sum_{i=1}^{N}L_iZ_i = A + B$$   \n",
    "\n",
    "Where, $ A = N(x| m_a, σ_a^2)$ and $ B = N(x | m_b, σ_b^2) $ and A,B independent. Therefore,   \n",
    "\n",
    "$$ A + B = N(x|μ,σ^2) $$   \n",
    "\n",
    "Thus, $\\textbf L \\cdot \\textbf X$ is a Gaussian Normal distribution and as a result $\\textbf X$ is a multivariate Gaussian random variable.\n",
    " \n",
    "Similarly to above, for independent random multivariate variables $X_1 = X$ and $X_2 = Y$, we will show that the distribution $f_Z$ of $\\boldsymbol\\ Z = \\boldsymbol\\ X_3 = \\boldsymbol\\ X + \\boldsymbol\\ Y = \\mathcal{N}(z |\\boldsymbol\\mu_X + \\boldsymbol\\mu_Y, \\boldsymbol\\Sigma_X + \\boldsymbol\\Sigma_X)$   \n",
    "We know that a Gaussian distribution is fully specified by its mean vector and covariance matrix. If we can determine what these are, then we are done.   \n",
    "For the mean:  \n",
    "<math>\n",
    "    \\begin{align}\n",
    "    E[x+y] = E[x] + E[y] = \\boldsymbol\\mu_X + \\boldsymbol\\mu_Y\n",
    "    \\end{align}\n",
    "</math>   \n",
    "Moreover, the $(i,j)th$ of the covariance matrix is given by:  \n",
    "\n",
    "$E[(x_i + y_i)(x_j + y_j)] - E[x_i + y_i]E[x_j + y_j] = $  \n",
    "$=E[x_ix_j + y_ix_j + x_iy_j + y_iy_j] − (E[x_i] + E[y_i])(E[x_j] + E[y_j])=$  \n",
    "$=E[x_ix_j] + E[y_ix_j] + E[x_iy_j] + E[y_iy_j] − E[x_i]E[x_j] − E[y_i]E[x_j] − E[x_i]E[y_j] − E[y_i][y_j]=$\n",
    "$=(E[x_ix_j] − E[x_i]E[x_j]) + (E[y_iy_j] − E[y_i]E[y_j]) + (E[y_ix_j] − E[y_i]E[x_j]) + (E[x_iy_j] − E[x_i]E[y_j])$\n",
    "   \n",
    "Using the fact that y and z are independent, we have: \n",
    "$E[y_ix_j] = E[y_i]E[x_j]$ and\n",
    "$E[x_iy_j] = E[x_i]E[y_j]$, as well.  \n",
    "Thus, the last two terms drop out, and we have:  \n",
    "$E[(x_i + y_i)(x_j + y_j)] - E[x_i + y_i]E[x_j + y_j] = $  \n",
    "$=(E[x_ix_j] − E[x_i]E[x_j]) + (E[y_iy_j] − E[y_i]E[y_j]) = $   \n",
    "$= \\boldsymbol\\Sigma_x + \\boldsymbol\\Sigma_y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 (a) Bayes Decision Theory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following Cauchy distribution:  \n",
    "$$ p(x|ω_i) = \\frac{1}{πb}\\cdot \\frac{1}{1 + \\frac{(x-a_i)^2}{b^2}} = \\frac{1}{π}\\cdot \\frac{b}{b^2+ (x-a_i)^2}, i=1,2$$\n",
    "We must show that its integral equals 1. Let θ represent the angle that a line, with fixed point of rotation, makes with the vertical axis. So:  \n",
    "$$ tanθ = \\frac{x-a}{b} $$  \n",
    "\n",
    "$$ θ = tan^{-1}\\left(\\frac{x-a}{b}\\right)$$  \n",
    "$$ dθ = \\frac{1}{1 + \\frac{(x-a)^2}{b^2}} \\frac{dx}{b}$$   \n",
    "  \n",
    "  \n",
    "$$ dθ = \\frac {b \\cdot dx}{b^2 + (x-a)^2} $$  \n",
    "  \n",
    "Thus, the distribution of angle θ is given by:   \n",
    "\n",
    "$$ \\frac {dθ}{π} = \\frac{1}{π}\\frac{b\\cdot dx}{b^2+(x-a)^2} $$  \n",
    "However, this is normalized over all angles, since  \n",
    "\n",
    "$$ \\int_{-π/2}^{π/2} \\frac{dθ}{π} = 1$$ and   \n",
    "\n",
    "$$ \\int_{-\\infty}^{\\infty} \\frac{1}{π}\\frac{b\\cdot dx}{b^2+(x-a)^2} = \\frac{1}{π}\\left[tan^{-1}\\left(\\frac{x-a}{b}\\right)\\right]_{-\\infty}^{\\infty} = \\frac{1}{π} \\left[\\frac{1}{2}π-\\left(-\\frac{1}{2}π\\right)\\right] = 1 $$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 (b)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the a priori probabilities $P(ω_1)=P(ω_2)$, we would show that $P(ω_1|x)=P(ω_2|x)$, if $x=\\frac{a_1+a_2}{2}$, that is the decision boundary, that minimizes the error, is the average of the maximum locations of the two Cauchy distributions.  \n",
    "Let $g_1(x)$ and $g_2(x)$ be the discriminative functions of the two categories, $ω_1$ and $ω_2$.  \n",
    "Specifically,  \n",
    "$$ g_1(x) = p(x|ω_1) p(ω_1) $$\n",
    "$$ g_2(x) = p(x|ω_2) p(ω_2) $$  \n",
    "\n",
    "In order to find the decision boundary we should solve the below equation:  \n",
    "$$ g_1(x) = g_2(x) $$   \n",
    "$$ p(x|ω_1) p(ω_1) = p(x|ω_2)p(ω_2) $$  \n",
    "$$ p(x|ω_1) = p(x|ω_2) $$   \n",
    "$$ \\frac{1}{πb}\\cdot\\frac{b}{b^2 + (x-a_1)^2} = \\frac{1}{πb}\\cdot\\frac{b}{b^2 + (x-a_2)^2} $$  \n",
    "$$ b^2 + (x-a_1)^2 = b^2 + (x-a_2)^2 $$   \n",
    "$$ (a_2-a_1)(x-a_1-a_2) = 0 $$   \n",
    "But, $a_1 \\neq a_2$  in order the two categories, $ω_1$, $ω_2$, to be discriminative.  \n",
    "So,  \n",
    "$$ x-a_1-a_2=0$$  \n",
    "$$ x = \\frac{a_1+a_2}{2} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 (c) Misclassification rate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute $P(error)$, we could compute first $P(correct)$. Then, $$P(error) = 1 - P(correct)$$  \n",
    "Let $a_1 < a_2$, given that $p(ω_1) = p(ω_2) = 1/2$\n",
    "$$ P(correct) = \\sum_{k=1}^{2} p(x\\in R_k,ω_κ) = \\sum_{k=1}^{2} \\int_{R_k} p(x\\in R_k,ω_κ)dx = $$  \n",
    "$$ \\sum_{k=1}^{2} \\int_{R_k} p(x\\in R_k|ω_κ)p(ω_κ)dx = $$  \n",
    "$$ \\frac{1}{2}\\int_{-\\infty}^{\\frac{a_1+a_2}{2}} \\frac{1}{πb}\\cdot\\frac{b}{b^2 + (x-a_1)^2}dx + \n",
    "     \\frac{1}{2}\\int_{\\frac{a_1+a_2}{2}}^{\\infty} \\frac{1}{πb}\\cdot\\frac{b}{b^2 + (x-a_2)^2}dx = $$  \n",
    "$$ \\frac{1}{2}\\frac{1}{π}\\left[tan^{-1}\\left(\\frac{x-a_1}{b}\\right)\\right]_{-\\infty}^{\\frac{a_1+a_2}{2}}\n",
    "   + \\frac{1}{2}\\frac{1}{π}\\left[tan^{-1}\\left(\\frac{x-a_2}{b}\\right)\\right]_{\\frac{a_1+a_2}{2}}^{\\infty} = $$  \n",
    "$$ \\frac{1}{2}\\frac{1}{π} \\left[tan^{-1}\\left(\\frac{a_2-a_1}{2b}\\right)+\\frac{π}{2}\n",
    "+  \\frac{π}{2} + tan^{-1}\\left(\\frac{a_2-a_1}{2b}\\right)\\right] = $$  \n",
    "$$ \\frac{1}{2} + \\frac{1}{π}tan^{-1}\\left(\\frac{a_2-a_1}{2b}\\right) $$  \n",
    "  \n",
    "Thus,\n",
    "$$P(error) = 1 - P(correct) = \\frac{1}{2} - \\frac{1}{π}tan^{-1}\\left(\\frac{a_2-a_1}{2b}\\right) $$  \n",
    "\n",
    "We can write the general form of the above form, considering $b>0$,\n",
    "$$P(error) = 1 - P(correct) = \\frac{1}{2} - \\frac{1}{π}tan^{-1}\\left(\\frac{|a_2-a_1|}{2b}\\right) $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 (a) Mahalanobis distance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the variables\n",
    "$$ \\textbf z = \\textbf x - \\textbf μ $$  \n",
    "\n",
    "$$ \\textbf B = \\textbf Σ^{-1}$$  \n",
    "\n",
    "Thus, the Mahalanobis distance can be written as\n",
    "\n",
    "$$ {r_i}^2 = \\textbf z^T \\textbf B \\textbf z $$  \n",
    "\n",
    "Thus, \n",
    "\n",
    "$$ \\nabla {r_i}^2 = \n",
    "\\frac{d(\\textbf z^T \\textbf B \\textbf z)}\n",
    "{d\\textbf z} $$  \n",
    "\n",
    "Define\n",
    "$$ \\textbf y = \\textbf B \\textbf z $$  \n",
    "  \n",
    "So, the gradient of the Mahalanobis distance in terms of $\\textbf z$ can be written   \n",
    "\n",
    "$$ \\nabla {r_i}^2 = \n",
    "\\frac{\\partial(\\textbf z^T \\textbf y)}\n",
    "{\\partial \\textbf z} \n",
    "+ \\frac{d\\left(\\textbf y(\\textbf z)^T\\right)}\n",
    "{d\\textbf z} \n",
    "\\frac{\\partial(\\textbf z^T \\textbf y)}\n",
    "{\\partial \\textbf y}$$  \n",
    "$$ = \\textbf y + \\frac{d(\\textbf z^T \\textbf B^T)}{d\\textbf z} = \\textbf y + \\textbf B^T \\textbf z $$   \n",
    "\n",
    "$$ = (\\textbf B + \\textbf B^T) \\textbf z $$  \n",
    "$$ = 2\\textbf B \\textbf z = 2\\textbf Σ^{-1} (\\textbf x - \\textbf μ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 (b) Gradient of the Mahalanobis distance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the equation of a line through a given point, $\\textbf μ_i$, and parallel to a given vector $\\textbf a$  \n",
    "$$ \\textbf x = \\textbf μ_i + λ \\textbf a $$\n",
    "Thus,  \n",
    "$$ \\textbf x - \\textbf μ_i  = λ \\textbf a $$  \n",
    "So,  \n",
    "$$ \\nabla {r_i}^2 = 2 \\textbf Σ^{-1}(\\textbf x - \\textbf μ_i) = 2 \\textbf Σ^{-1} λ \\textbf a = \n",
    "2 λ \\textbf Σ^{-1} \\textbf a $$  \n",
    "Therefore, $\\nabla {r_i}^2$ has a constant direction, but it is not parallel to $\\textbf a$, since it is transformed by $\\textbf Σ^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 (c)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the vector line equation through $\\textbf μ_1$ and $\\textbf μ_2$:   \n",
    "$$ \\textbf x = \\textbf μ_1 + l(\\textbf μ_1 - \\textbf μ_2) $$  \n",
    "Moreover, we can write the above equation as:  \n",
    "$$ \\textbf x = \\textbf μ_2 + k(\\textbf μ_1 - \\textbf μ_2) $$   \n",
    "Subtracting the above equations and solving in terms of k:  \n",
    "\n",
    "$$ \\textbf0 = (\\textbf μ_1 - \\textbf μ_2) + (l-k)(\\textbf μ_1 - \\textbf μ_2) $$  \n",
    "$$ =>k = l + 1 $$  \n",
    "\n",
    "Solving the line equation for point $\\textbf μ_1$:  \n",
    "$$ \\textbf μ_1 = \\textbf μ_1 + l(\\textbf μ_1 - \\textbf μ_2) $$   \n",
    "$$ =>l = 0 $$   \n",
    "\n",
    "And Solving the line equation for point $\\textbf μ_2$:  \n",
    "$$ \\textbf μ_2 = \\textbf μ_1 + l(\\textbf μ_1 - \\textbf μ_2) $$   \n",
    "$$ => l = -1 $$  \n",
    "\n",
    "So, $l \\in (-1, 0)$. Then, calculating the gradients, we take:   \n",
    "$$ \\nabla r_1^2 = 2\\textbf Σ^{-1} l (\\textbf μ_1 - \\textbf μ_2)  $$\n",
    "$$ \\nabla r_2^2 = 2\\textbf Σ^{-1} k (\\textbf μ_1 - \\textbf μ_2)  $$  \n",
    "\n",
    "Then, since $ k = l + 1 $, we take:  \n",
    "$$ \\nabla r_1^2 = \\frac{l}{l+1} \\nabla r_1^2 $$  \n",
    "\n",
    "Therefore, since $l \\in (-1, 0)$, then $\\frac{l}{l+1} < 0$, and as a result $\\nabla r_1^2$ and $\\nabla r_1^2$ have opposite directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 (e)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a problem for two classes $ω_1$, $ω_2$ with multivariate Gaussian distributions, where $\\textbf μ_1 \\neq \\textbfμ_2$, $\\textbf Σ_1 \\neq \\textbf Σ_2$ and $P(ω_1)=P(ω_2)$. Then, the Bayes decision boundary includes all points that have the same Mahalanobis distances from $\\textbf μ_1 \\neq \\textbfμ_2$.   \n",
    "*Proof:*   \n",
    "We could find decision boundary by equalising $ P(ω_1|\\textbf x) = P(ω_2|\\textbf x) $:  \n",
    "\n",
    "$$ P(ω_1|\\textbf x) = P(ω_2|\\textbf x) $$   \n",
    "$$ ln(P(ω_1|\\textbf x)) = ln(P(ω_2|\\textbf x)) $$  \n",
    "$$ ln(P(ω_1)) + (\\textbf x - \\textbfμ_1)^T\\textbf Σ^{-1} (\\textbf x - \\textbfμ_1) = \n",
    "ln(P(ω_2)) + (\\textbf x - \\textbfμ_2)^T\\textbf Σ^{-1} (\\textbf x - \\textbfμ_2) $$   \n",
    "$$ (\\textbf x - \\textbf μ_1)^T\\textbf Σ^{-1} (\\textbf x - \\textbfμ_1) = (\\textbf x - \\textbfμ_2)^T\\textbf Σ^{-1} (\\textbf x - \\textbf μ_2) $$    \n",
    "$$ {r_1}^2 = {r_2}^2 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4 (a) Maximum Likelihood estimation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $p(x|θ) = U(0,θ) = \\frac{1}{θ}$, for $x \\in [0,θ]$ and $0$ elsewhere.  \n",
    "Then,  \n",
    "$$L(θ,\\textbf x) = p(\\textbf x |θ) = \n",
    "\\prod_{i=1}^{n} \\frac{1}{θ} = θ^{-n}$$   \n",
    "since $\\textbf x$ is iid.   \n",
    "\n",
    "Taking the derivative of ln likelihood:   \n",
    "$$ \\frac{d\\left(ln L(θ|\\textbf x)\\right)}{dθ} = -\\frac{n}{θ} < 0$$  \n",
    "\n",
    "We want to find the value of θ, so that $L(θ,\\textbf x)$ becomes maximum.  \n",
    "Therefore, \n",
    "$$ θ_{ML} = argmax_θ L(θ,\\textbf x)$$   \n",
    "\n",
    "It is clear that $L(θ,\\textbf x)$ is a decreasing function for $θ\\geq max_ix_i$ . So, the likelihood function is maximized, when θ becomes $max_ix_i$.  \n",
    "Thus,   \n",
    "$$ θ_{ML} = max_ix_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4 (b)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $ n = 5$ and $ max_ix_i = 0.6$,   \n",
    "\n",
    "$$L(θ,\\textbf x) = p(\\textbf x |θ) = \n",
    "\\prod_{i=1}^{n} \\frac{1}{θ} = θ^{-n} = θ^{-5}$$   \n",
    "\n",
    "Thus,  \n",
    "\n",
    "$$  θ_{ML} = max_ix_i = 0.6 $$  \n",
    "\n",
    "So, it is not necessary to know any value of the {$x_i$}, other than the max value. Therefore, we should plot the likelihood function for $ θ \\geq θ_{ML} = 0.6$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ml.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5 (a) k-Nearest Neihbors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that for the two categories, $ω_1$ and $ω_2$, $p(\\textbf x | ω_i)$ are uniform distributions inside unit hyperspheres at a distance of 10 units and samples belong to either category with probability $p(ω_i) = \\frac{1}{2}$, we note that k-Nearest Neihbors algorithm should perform really well on this classification problem, when there are plenty of samples in both categories, that is inside both hyperspheres, since they are not close to each other.  \n",
    "More specifically, given an odd value of k, in order  k-Nearest Neihbors to avoid ties, the algorithm might predict wrong, when there is not a sufficient number of training samples in both classes. This sufficient number depends on the value of k. Since k-Nearest Neihbors needs the aspect of the majority of the k nearest neighbors, for values of number of samples belonging to one category between $0$ and $\\frac{k-1}{2}$, that is the minority of the nearest k neighbors, the algorithm predicts that a new sample, that belongs to that category, belongs to the other one, and as a result, it fails.   \n",
    "Therefore, the mean error probability for this classification problem is a sum of  binomial distributions   \n",
    "\n",
    "$$ P_n(e)= \\sum_{j=0}^{\\frac{k-1}{2}} {{n \\choose j} p(ω_1)^j (1-p(ω_1))^{n-j}}= \n",
    "\\frac{1}{2^n} \\sum_{j=0}^{\\frac{k-1}{2}} {n \\choose j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5 (b)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $k=1$, the error probability equals the probability for n number of samples, all to belong to the same category. \n",
    "Therefore   \n",
    "\n",
    "$$ P_n(e) = \\frac {1}{2^n} < \\frac{1}{2^n} \\sum_{j=0}^{\\frac{k-1}{2}} {n \\choose j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5 (c)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{1}{2^n} \\sum_{j=0}^{a\\sqrt{n}}{n \\choose j} \\leq \\frac{a\\sqrt{n}\\cdot n^{a\\sqrt{n}}}{2^n} =\n",
    "a\\sqrt{n} \\frac {2^{a\\sqrt{n}logn}}{2^n} \n",
    "$$ \n",
    "  \n",
    "$$ = \\frac{1}{ 2^{n-a\\sqrt{n}logn-\\frac{1}{2}logn-loga } } \n",
    "$$   \n",
    "\n",
    "\n",
    "which lim equals 0 when $n \\to \\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.6 Perceptrons**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we define the perceptron model:   \n",
    "\n",
    "- $\\textbf w(t+1) = \\textbf w(t) + p \\textbf x_t$, if $\\textbf x_t \\in ω_1$ and $\\textbf w(t)^T \\textbf x_t \\leq 0$   \n",
    "\n",
    "- $\\textbf w(t+1) = \\textbf w(t) - p \\textbf x_t$, if $\\textbf x_t \\in ω_2$ and $\\textbf w(t)^T \\textbf x_t \\geq 0$    \n",
    "\n",
    "- $ \\textbf w(t+1) = \\textbf w(t) $, otherwise   \n",
    "\n",
    "Given 10 samples; 5 of $ω_1$ and 5 of $ω_2$, and $p=1$, we should note if they are linearly discriminant.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"perceptron.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, we note that they are linearly discriminant.   \n",
    "However, with initial $ \\textbf w(0) = [0,0]^T $, since the samples given are unbiased, perceptron algorithm will never converge, and will never return a result $ \\textbf w $, because it seeks for a line equation: $ ax + by = 0$.   \n",
    "Thus, we should add the bias term as follows:   \n",
    "\n",
    "$ ω_1: [1,-1,4]^T,[1,1,2]^T,[1,2,-2]^T,[1,1,-4]^T,[1,4,-1]^T$    \n",
    "\n",
    "$ ω_2: [1,-4, 2]^T,[1,-2,1]^T,[1,-2,-1]^T,[1,-1,-3]^T,[1,-1,-6]^T$.  \n",
    "\n",
    "Then, the algorithm would seek for a line equation: $ c + ax + by = 0 $, where $c$ is the bias term, and will converge, with initial $ \\textbf w(0) = [0,0,0]^T $.  \n",
    "More specifically,   \n",
    "\n",
    "for $t=0$: $\\textbf w(0) = [0,0,0]^T, \\textbf x_0 = [1,-1,4], \\textbf w(1) = [1,-1,4]$,   \n",
    "for $t=1$: $\\textbf w(1) = [1,-1,4]^T, \\textbf x_1 = [1,1,2], \\textbf w(2) = [1,-1,4]$, etc.   \n",
    "\n",
    "And, we end up with $\\textbf w = [5,7,1]^T$. Therefore, the line equation is $5 + 7x + y = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.7 EM on GMMs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Christopher M.Bishop EM algorithm for GMMs in order to maximize the likelihood function\n",
    "with respect to the parameters, comprising the means and covariances of the\n",
    "components and the mixing coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "μ = \t [3.00102979 0.98464715 2.00536903]\n",
      "var = \t [0.00980393 0.01158503 0.06829099]\n",
      "Pi = \t [0.4951724  0.24596795 0.25885965]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "# Dataset \n",
    "samples = []\n",
    "for i in range(500):\n",
    "    mod=i%4\n",
    "    if mod in [0,1]:\n",
    "        samples.append(np.random.normal(3, 0.1))\n",
    "    elif (mod == 2):\n",
    "        samples.append(np.random.normal(1, 0.1))\n",
    "    else:\n",
    "        samples.append(np.random.normal(2, 0.2))\n",
    "\n",
    "# n_samples\n",
    "N = len(samples)\n",
    "\n",
    "# n_classes\n",
    "K = 3\n",
    "\n",
    "# Initialization of parameters\n",
    "stds = [0.2, 0.3, 1]\n",
    "means = [3.1, 0.7, 2.5]\n",
    "probs = [0.5, 0.15, 0.15]\n",
    "const = 0.0000000001\n",
    "\n",
    "\n",
    "def normal(x, mean, std):\n",
    "    # Normal Distribution\n",
    "    return scipy.stats.norm(mean, std).pdf(x)\n",
    "\n",
    "\n",
    "def ll(gammas):\n",
    "    temp = 0\n",
    "    for n in range(N):\n",
    "        temp += np.log((sum(gammas[n,:]) + const))\n",
    "    return temp  \n",
    "\n",
    "# Rounds of EM algorithm\n",
    "i = 0\n",
    "cur_likelihood=-1500\n",
    "\n",
    "## EM algorithm ##\n",
    "while(True):\n",
    "    i += 1\n",
    "    \n",
    "    # E_STEP\n",
    "    gammas = np.zeros((N,K))\n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            gammas[n][k] = float(probs[k] * normal(samples[n], means[k], stds[k] + const))\n",
    "        norm_factor=float(sum(gammas[n,:]))\n",
    "        for k in range(K):\n",
    "            gammas[n][k] /= float((norm_factor + const))\n",
    "            \n",
    "    # M_STEP       \n",
    "    next_means = np.zeros((K))\n",
    "    next_stds = np.zeros((K))\n",
    "    next_probs = np.zeros((K))    \n",
    "    Nk = np.zeros((K))\n",
    "    for k in range(K):\n",
    "        Nk[k] = float(sum(gammas[:,k]) + const)\n",
    "    for k in range(K):\n",
    "        next_means[k] = float(np.dot(np.squeeze(gammas[:,k]), samples) / Nk[k])\n",
    "        next_stds[k] = float(np.sqrt(np.dot(np.squeeze(gammas[:,k]), (samples - next_means[k])**2 / Nk[k])))\n",
    "        next_probs[k] = float(Nk[k]/N)   \n",
    "    \n",
    "    # Update means, stds, pi\n",
    "    means, stds, probs = np.array(next_means), np.array(next_stds), np.array(next_probs)\n",
    "    \n",
    "    # Compute new likelihood\n",
    "    next_likelihood = ll(gammas)\n",
    "    \n",
    "    # Loss Function\n",
    "    if (np.abs(next_likelihood - cur_likelihood) < 0.000000001):\n",
    "        break\n",
    "    cur_likelihood = next_likelihood\n",
    "\n",
    "print(\"μ = \\t\", means)\n",
    "print(\"var = \\t\", stds**2)\n",
    "print(\"Pi = \\t\", probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that EM's Pi output is incredibly close to the expected values of the dataset, having better results than k-means, which result in this problem for Pi is approximately [0.5, 0.27, 0.23]. Moreover, we could have set k-means results as EM inputs for a more efficient result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
