{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Βήμα 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
    "import re\n",
    "\n",
    "# Combine similar classes and remove underrepresented classes\n",
    "class_mapping = {\n",
    "    'Rock': 'Rock',\n",
    "    'Psych-Rock': 'Rock',\n",
    "    'Indie-Rock': None,\n",
    "    'Post-Rock': 'Rock',\n",
    "    'Psych-Folk': 'Folk',\n",
    "    'Folk': 'Folk',\n",
    "    'Metal': 'Metal',\n",
    "    'Punk': 'Metal',\n",
    "    'Post-Punk': None,\n",
    "    'Trip-Hop': 'Trip-Hop',\n",
    "    'Pop': 'Pop',\n",
    "    'Electronic': 'Electronic',\n",
    "    'Hip-Hop': 'Hip-Hop',\n",
    "    'Classical': 'Classical',\n",
    "    'Blues': 'Blues',\n",
    "    'Chiptune': 'Electronic',\n",
    "    'Jazz': 'Jazz',\n",
    "    'Soundtrack': None,\n",
    "    'International': None,\n",
    "    'Old-Time': None\n",
    "}\n",
    "\n",
    "# TODO: Comment on howv the train and validation splits are created.\n",
    "# TODO: It's useful to set the seed when debugging but when experimenting ALWAYS set seed=None. Why?\n",
    "def torch_train_val_split(dataset, batch_train, batch_eval,\n",
    "                            val_size=.2, shuffle=True, seed=None):\n",
    "    # Uses seed and shuffling on dataset and produces a train loader and a validation loader \n",
    "    # with defining batch size\n",
    "    \n",
    "    # Creating data indices for training and validation splits:\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    val_split = int(np.floor(val_size * dataset_size))\n",
    "    if shuffle:\n",
    "        np.random.seed(seed)  \n",
    "        np.random.shuffle(indices)  # kanw shuffle exontas arxikopoisei to seed, settarismeno sto debug prokeimenou na exw idio apotelesma\n",
    "        # indices px [1,4,0,2,5 ...]\n",
    "    \n",
    "    train_indices = indices[val_split:]\n",
    "    val_indices = indices[:val_split]\n",
    "\n",
    "    # Creating PT data samplers and loaders:\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = DataLoader(dataset,\n",
    "                              batch_size=batch_train,\n",
    "                              sampler=train_sampler)\n",
    "    val_loader = DataLoader(dataset,\n",
    "                            batch_size=batch_eval,\n",
    "                            sampler=val_sampler)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def read_fused_spectrogram(spectrogram_file):\n",
    "    '''\n",
    "        spectrogram_file:  String, path file\n",
    "    '''\n",
    "    spectrogram = np.load(spectrogram_file)\n",
    "    return spectrogram.T  # return (timesteps x freq)\n",
    "\n",
    "\n",
    "def read_mel_spectrogram(spectrogram_file):\n",
    "    '''\n",
    "        spectrogram_file:  String, path file\n",
    "    '''\n",
    "    spectrogram = np.load(spectrogram_file)[:128]\n",
    "    return spectrogram.T\n",
    "\n",
    "    \n",
    "def read_chromagram(spectrogram_file):\n",
    "    '''\n",
    "        spectrogram_file:  String, path file\n",
    "    '''\n",
    "    spectrogram = np.load(spectrogram_file)[128:]\n",
    "    return spectrogram.T\n",
    "\n",
    "\n",
    "class LabelTransformer(LabelEncoder):\n",
    "    def inverse(self, y):\n",
    "        try:\n",
    "            return super(LabelTransformer, self).inverse_transform(y)\n",
    "        except:\n",
    "            return super(LabelTransformer, self).inverse_transform([y])\n",
    "\n",
    "    def transform(self, y):\n",
    "        try:\n",
    "            return super(LabelTransformer, self).transform(y)\n",
    "        except:\n",
    "            return super(LabelTransformer, self).transform([y])\n",
    "\n",
    "\n",
    "# TODO: Comment on why padding is needed\n",
    "class PaddingTransform(object):\n",
    "    # Padding when needed\n",
    "    def __init__(self, max_length, padding_value=0):\n",
    "        self.max_length = max_length\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "    def __call__(self, s):\n",
    "        '''\n",
    "            s: numpy array\n",
    "        '''\n",
    "        if len(s) == self.max_length:\n",
    "            return s\n",
    "\n",
    "        if len(s) > self.max_length:\n",
    "            return s[:self.max_length] \n",
    "\n",
    "        if len(s) < self.max_length:\n",
    "            # Padding in order to have same numpy shapes for all samples\n",
    "            # since their timesteps dimension differs \n",
    "            s1 = copy.deepcopy(s)\n",
    "            pad = np.zeros((self.max_length - s.shape[0], s.shape[1]), dtype=np.float32)\n",
    "            s1 = np.vstack((s1, pad))\n",
    "            return s1\n",
    "\n",
    "        \n",
    "class SpectrogramDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path, class_mapping=None, train=True, max_length=-1, read_spec_fn=read_mel_spectrogram):\n",
    "        '''\n",
    "            path:            String, the filepath of our data\n",
    "            class_mapping:   Dict, the mapping of kinds of music\n",
    "            train:           Boolean, (training or testing)\n",
    "            max_length:      int\n",
    "            read_spec_fn     function, the way that we would like to use the data\n",
    "        '''\n",
    "        t = 'train' if train else 'test'\n",
    "        p = os.path.join(path, t)\n",
    "        self.index = os.path.join(path, \"{}_labels.txt\".format(t))\n",
    "        self.files, labels = self.get_files_labels(self.index, class_mapping)\n",
    "        self.feats = [read_spec_fn(os.path.join(p, f)) for f in self.files]\n",
    "        self.feat_dim = self.feats[0].shape[1]\n",
    "        self.lengths = [len(i) for i in self.feats]\n",
    "        self.max_length = max(self.lengths) if max_length <= 0 else max_length\n",
    "        self.zero_pad_and_stack = PaddingTransform(self.max_length)\n",
    "        self.label_transformer = LabelTransformer()\n",
    "        if isinstance(labels, (list, tuple)):\n",
    "            self.labels = np.array(self.label_transformer.fit_transform(labels)).astype('int64')\n",
    "\n",
    "            \n",
    "    def get_files_labels(self, txt, class_mapping):\n",
    "        # Returns a list of file names and a list of their labels\n",
    "        with open(txt, 'r') as fd:\n",
    "            lines = [l.rstrip().split('\\t') for l in fd.readlines()[1:]]\n",
    "        files, labels = [], []\n",
    "        for l in lines:\n",
    "            label = l[1]\n",
    "            if class_mapping:\n",
    "                label = class_mapping[l[1]]\n",
    "            if label is None:\n",
    "                continue\n",
    "            # Kaggle automatically unzips the npy.gz format so this hack is needed\n",
    "            _id = l[0].split('.')[0]\n",
    "            npy_file = '{}.fused.full.npy'.format(_id)\n",
    "            files.append(npy_file)\n",
    "            labels.append(label)\n",
    "            \n",
    "        return files, labels\n",
    "\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        # TODO: Inspect output and comment on how the output is formatted\n",
    "        # Returns padded item, item label, l\n",
    "        l = min(self.lengths[item], self.max_length)\n",
    "        return self.zero_pad_and_stack(self.feats[item]), self.labels[item], l\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Returns the number of dataset samples\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ##################################################################################\n",
    "    # load single synced mel spectrograms\n",
    "    ##################################################################################\n",
    "    # Dataset\n",
    "    mel_specs = SpectrogramDataset('../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/', train=True,\n",
    "                                        class_mapping=class_mapping, max_length=-1,\n",
    "                                        read_spec_fn=read_mel_spectrogram)\n",
    "    # Train and Test loaders\n",
    "    train_loader_mel, val_loader_mel = torch_train_val_split(mel_specs, 32, 32, val_size=.33)\n",
    "    test_dataset_mel = SpectrogramDataset('../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/', train=False,\n",
    "                                            class_mapping=class_mapping, max_length=-1,\n",
    "                                            read_spec_fn=read_mel_spectrogram)\n",
    "    \n",
    "    test_loader_mel = DataLoader(test_dataset_mel, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "    \n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=5),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "        )    \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, kernel_size=5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),    \n",
    "            \n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 64, kernel_size=5),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 256, kernel_size=5),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "        )\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "        )\n",
    "#         self.fc1 = nn.Linear(6160, 10)\n",
    "        self.fc1 = nn.Linear(19712*4, 10)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward function of CNN\n",
    "        \n",
    "        out = self.layer1(x)     # conv1\n",
    "        out = self.layer2(out)   # conv2\n",
    "        out = self.layer3(out)   # conv3\n",
    "        out = self.layer4(out)   # conv4\n",
    "#         out = self.layer(x) \n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = nn.Dropout2d(p=0.28)(out)\n",
    "        out = self.fc1(out)      # fully-connected1\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_CNN(train_loader, val_loader, val_name, mels, dataset):\n",
    "    '''\n",
    "        train_loader:  Dataloader\n",
    "        val_loader:    Dataloader\n",
    "        val_name:      String\n",
    "        mels:          int, input size\n",
    "        dataset:       SpectrumDataset\n",
    "    '''\n",
    "    \n",
    "    val_name += \".pkl\"\n",
    "    \n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Hyper-parameters\n",
    "    num_classes = 10\n",
    "    num_epochs = 50\n",
    "    learning_rate = 0.0001\n",
    "    momentum = 0\n",
    "    \n",
    "    maxseqlen = dataset.max_length\n",
    "\n",
    "    model = CNN().to(device)\n",
    "    model = model.double()\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "    # Train the model\n",
    "    train_loss = []\n",
    "    vl = []\n",
    "    tl = []\n",
    "    min_val_loss = np.Inf\n",
    "    n_epochs_stop = 10000000\n",
    "    epochs_no_improve = 0\n",
    "    best_val_loss = np.Inf\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss.append([])\n",
    "        val_loss = 0\n",
    "        early_stop = False\n",
    "\n",
    "\n",
    "        for i, (feats, labels, lengths) in enumerate(val_loader):\n",
    "            # Validation set\n",
    "            feats_reshape = feats.reshape(-1, 1, maxseqlen, mels).to(device)\n",
    "            labels = labels.to(device) \n",
    "            labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model.forward(feats_reshape)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        vl.append(val_loss / i)\n",
    "        print(\"Validation loss:\\t\", val_loss)\n",
    "        \n",
    "        if val_loss <= best_val_loss:\n",
    "            # Keep the model with minimum validation loss\n",
    "            best_val_loss = val_loss\n",
    "            joblib_file = val_name  \n",
    "            joblib.dump(model, joblib_file)\n",
    "\n",
    "        v_loss = 0\n",
    "        for i, (feats, labels, lengths) in enumerate(train_loader):\n",
    "            # Training set\n",
    "            feats_reshape = feats.reshape(-1, 1, maxseqlen, mels).to(device)\n",
    "            labels = labels.to(device) \n",
    "            labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model.forward(feats_reshape)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i) % 4 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            train_loss[epoch].append(loss.item())\n",
    "\n",
    "            # epoch loss\n",
    "            v_loss += loss\n",
    "            v_loss = v_loss / len(train_loader)\n",
    "\n",
    "            # If the validation loss is at a minimum\n",
    "            if v_loss < min_val_loss:\n",
    "\n",
    "                #torch.save(model)\n",
    "                epochs_no_improve = 0\n",
    "                min_val_loss = v_loss\n",
    "\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "            # Check early stopping condition           \n",
    "            if epochs_no_improve == n_epochs_stop:\n",
    "                print('Early stopping!' )\n",
    "                early_stop = True\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "        if early_stop:\n",
    "            print(\"Stopped\")\n",
    "            break\n",
    "            \n",
    "        tl.append(np.mean(train_loss[epoch]))\n",
    "\n",
    "    # Tranform losses to numpy arrays\n",
    "    tl=np.array(tl)\n",
    "    vl=np.array(vl)\n",
    "        \n",
    "    # Plotting learning curve\n",
    "    plt.figure()\n",
    "    plt.plot(tl, label=\"training loss\")\n",
    "    plt.plot(vl, label=\"validation loss\")\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('mean loss in the epoch')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.show()\n",
    "    \n",
    "    return maxseqlen\n",
    "    \n",
    "            \n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "def score_CNN(test_loader, file_model, mels, maxseqlen):\n",
    "    # Estimating the model with classification report\n",
    "    '''\n",
    "        test_loader:   Dataloader\n",
    "        file_model:    String, filepath of best model\n",
    "        mels:          int, input size\n",
    "        maxseqlen:     int\n",
    "    '''\n",
    "        \n",
    "    file_model += \".pkl\"\n",
    "    model = joblib.load(file_model)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Disable batch normalization and dropout in testing\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for feats, labels, lengths in test_loader:\n",
    "            feats = feats.reshape(-1, 1, maxseqlen, mels).to(device)\n",
    "            labels = labels.to(device)\n",
    "            labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "            outputs = model.forward(feats).double()\n",
    "            _, predicted = torch.max(outputs.data, -1)\n",
    "            y_pred.append(predicted.item())\n",
    "            y_true.append(labels.item())\n",
    "\n",
    "        print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:\t 63.5076498635987\n",
      "Epoch [1/50], Step [1/49], Loss: 2.7471\n",
      "Epoch [1/50], Step [5/49], Loss: 2.4395\n",
      "Epoch [1/50], Step [9/49], Loss: 2.3369\n",
      "Epoch [1/50], Step [13/49], Loss: 2.2404\n",
      "Epoch [1/50], Step [17/49], Loss: 2.1593\n",
      "Epoch [1/50], Step [21/49], Loss: 2.3323\n",
      "Epoch [1/50], Step [25/49], Loss: 2.0074\n",
      "Epoch [1/50], Step [29/49], Loss: 2.1787\n",
      "Epoch [1/50], Step [33/49], Loss: 2.0195\n",
      "Epoch [1/50], Step [37/49], Loss: 2.2525\n",
      "Epoch [1/50], Step [41/49], Loss: 2.1601\n",
      "Epoch [1/50], Step [45/49], Loss: 1.9969\n",
      "Epoch [1/50], Step [49/49], Loss: 2.1518\n",
      "Validation loss:\t 57.56088943499755\n",
      "Epoch [2/50], Step [1/49], Loss: 2.4159\n",
      "Epoch [2/50], Step [5/49], Loss: 2.2549\n",
      "Epoch [2/50], Step [9/49], Loss: 1.9848\n",
      "Epoch [2/50], Step [13/49], Loss: 2.1280\n",
      "Epoch [2/50], Step [17/49], Loss: 2.0748\n",
      "Epoch [2/50], Step [21/49], Loss: 1.6829\n",
      "Epoch [2/50], Step [25/49], Loss: 1.9419\n",
      "Epoch [2/50], Step [29/49], Loss: 1.9690\n",
      "Epoch [2/50], Step [33/49], Loss: 1.7793\n",
      "Epoch [2/50], Step [37/49], Loss: 1.9389\n",
      "Epoch [2/50], Step [41/49], Loss: 1.8749\n",
      "Epoch [2/50], Step [45/49], Loss: 2.1290\n",
      "Epoch [2/50], Step [49/49], Loss: 2.0509\n",
      "Validation loss:\t 50.7984744612096\n",
      "Epoch [3/50], Step [1/49], Loss: 2.0835\n",
      "Epoch [3/50], Step [5/49], Loss: 1.7665\n",
      "Epoch [3/50], Step [9/49], Loss: 1.5899\n",
      "Epoch [3/50], Step [13/49], Loss: 1.8798\n",
      "Epoch [3/50], Step [17/49], Loss: 1.6338\n",
      "Epoch [3/50], Step [21/49], Loss: 1.8052\n",
      "Epoch [3/50], Step [25/49], Loss: 2.0045\n",
      "Epoch [3/50], Step [29/49], Loss: 2.0071\n",
      "Epoch [3/50], Step [33/49], Loss: 2.0116\n",
      "Epoch [3/50], Step [37/49], Loss: 1.8073\n",
      "Epoch [3/50], Step [41/49], Loss: 1.9262\n",
      "Epoch [3/50], Step [45/49], Loss: 2.1058\n",
      "Epoch [3/50], Step [49/49], Loss: 2.4346\n",
      "Validation loss:\t 49.93166326163302\n",
      "Epoch [4/50], Step [1/49], Loss: 1.5854\n",
      "Epoch [4/50], Step [5/49], Loss: 2.0487\n",
      "Epoch [4/50], Step [9/49], Loss: 1.5944\n",
      "Epoch [4/50], Step [13/49], Loss: 1.6517\n",
      "Epoch [4/50], Step [17/49], Loss: 1.8251\n",
      "Epoch [4/50], Step [21/49], Loss: 1.9344\n",
      "Epoch [4/50], Step [25/49], Loss: 1.7066\n",
      "Epoch [4/50], Step [29/49], Loss: 1.9397\n",
      "Epoch [4/50], Step [33/49], Loss: 1.8976\n",
      "Epoch [4/50], Step [37/49], Loss: 1.6700\n",
      "Epoch [4/50], Step [41/49], Loss: 1.6464\n",
      "Epoch [4/50], Step [45/49], Loss: 1.7090\n",
      "Epoch [4/50], Step [49/49], Loss: 1.7781\n",
      "Validation loss:\t 49.30210034323362\n",
      "Epoch [5/50], Step [1/49], Loss: 1.5330\n",
      "Epoch [5/50], Step [5/49], Loss: 1.6810\n",
      "Epoch [5/50], Step [9/49], Loss: 1.8699\n",
      "Epoch [5/50], Step [13/49], Loss: 1.7110\n",
      "Epoch [5/50], Step [17/49], Loss: 1.7182\n",
      "Epoch [5/50], Step [21/49], Loss: 1.6657\n",
      "Epoch [5/50], Step [25/49], Loss: 1.6696\n",
      "Epoch [5/50], Step [29/49], Loss: 1.6663\n",
      "Epoch [5/50], Step [33/49], Loss: 1.9889\n",
      "Epoch [5/50], Step [37/49], Loss: 1.3758\n",
      "Epoch [5/50], Step [41/49], Loss: 2.2518\n",
      "Epoch [5/50], Step [45/49], Loss: 2.0677\n",
      "Epoch [5/50], Step [49/49], Loss: 1.2353\n",
      "Validation loss:\t 48.8367071114771\n",
      "Epoch [6/50], Step [1/49], Loss: 1.8325\n",
      "Epoch [6/50], Step [5/49], Loss: 1.6978\n",
      "Epoch [6/50], Step [9/49], Loss: 1.4084\n",
      "Epoch [6/50], Step [13/49], Loss: 1.4290\n",
      "Epoch [6/50], Step [17/49], Loss: 1.6332\n",
      "Epoch [6/50], Step [21/49], Loss: 1.6041\n",
      "Epoch [6/50], Step [25/49], Loss: 1.5594\n",
      "Epoch [6/50], Step [29/49], Loss: 1.6526\n",
      "Epoch [6/50], Step [33/49], Loss: 1.8680\n",
      "Epoch [6/50], Step [37/49], Loss: 1.5091\n",
      "Epoch [6/50], Step [41/49], Loss: 1.5264\n",
      "Epoch [6/50], Step [45/49], Loss: 1.5242\n",
      "Epoch [6/50], Step [49/49], Loss: 2.2147\n",
      "Validation loss:\t 48.68952458779725\n",
      "Epoch [7/50], Step [1/49], Loss: 1.7025\n",
      "Epoch [7/50], Step [5/49], Loss: 1.7110\n",
      "Epoch [7/50], Step [9/49], Loss: 1.4299\n",
      "Epoch [7/50], Step [13/49], Loss: 1.4606\n",
      "Epoch [7/50], Step [17/49], Loss: 1.6213\n",
      "Epoch [7/50], Step [21/49], Loss: 1.3292\n",
      "Epoch [7/50], Step [25/49], Loss: 1.8374\n",
      "Epoch [7/50], Step [29/49], Loss: 1.6572\n",
      "Epoch [7/50], Step [33/49], Loss: 1.7226\n",
      "Epoch [7/50], Step [37/49], Loss: 1.5723\n",
      "Epoch [7/50], Step [41/49], Loss: 1.6679\n",
      "Epoch [7/50], Step [45/49], Loss: 1.4883\n",
      "Epoch [7/50], Step [49/49], Loss: 1.5963\n",
      "Validation loss:\t 47.655823490746144\n",
      "Epoch [8/50], Step [1/49], Loss: 1.3302\n",
      "Epoch [8/50], Step [5/49], Loss: 1.4593\n",
      "Epoch [8/50], Step [9/49], Loss: 1.4934\n",
      "Epoch [8/50], Step [13/49], Loss: 1.4789\n",
      "Epoch [8/50], Step [17/49], Loss: 1.5415\n",
      "Epoch [8/50], Step [21/49], Loss: 1.3194\n",
      "Epoch [8/50], Step [25/49], Loss: 1.6506\n",
      "Epoch [8/50], Step [29/49], Loss: 1.4068\n",
      "Epoch [8/50], Step [33/49], Loss: 1.4487\n",
      "Epoch [8/50], Step [37/49], Loss: 1.2985\n",
      "Epoch [8/50], Step [41/49], Loss: 1.6058\n",
      "Epoch [8/50], Step [45/49], Loss: 1.5650\n",
      "Epoch [8/50], Step [49/49], Loss: 1.6978\n",
      "Validation loss:\t 53.17694518131372\n",
      "Epoch [9/50], Step [1/49], Loss: 1.8203\n",
      "Epoch [9/50], Step [5/49], Loss: 1.3798\n",
      "Epoch [9/50], Step [9/49], Loss: 1.8864\n",
      "Epoch [9/50], Step [13/49], Loss: 1.4249\n",
      "Epoch [9/50], Step [17/49], Loss: 1.4671\n",
      "Epoch [9/50], Step [21/49], Loss: 1.5113\n",
      "Epoch [9/50], Step [25/49], Loss: 1.0813\n",
      "Epoch [9/50], Step [29/49], Loss: 1.5759\n",
      "Epoch [9/50], Step [33/49], Loss: 1.4241\n",
      "Epoch [9/50], Step [37/49], Loss: 1.3346\n",
      "Epoch [9/50], Step [41/49], Loss: 1.6584\n",
      "Epoch [9/50], Step [45/49], Loss: 1.2897\n",
      "Epoch [9/50], Step [49/49], Loss: 1.7223\n",
      "Validation loss:\t 46.49054660599707\n",
      "Epoch [10/50], Step [1/49], Loss: 1.4537\n",
      "Epoch [10/50], Step [5/49], Loss: 1.6678\n",
      "Epoch [10/50], Step [9/49], Loss: 1.2999\n",
      "Epoch [10/50], Step [13/49], Loss: 1.5062\n",
      "Epoch [10/50], Step [17/49], Loss: 1.4223\n",
      "Epoch [10/50], Step [21/49], Loss: 1.5267\n",
      "Epoch [10/50], Step [25/49], Loss: 1.6684\n",
      "Epoch [10/50], Step [29/49], Loss: 1.4485\n",
      "Epoch [10/50], Step [33/49], Loss: 1.2939\n",
      "Epoch [10/50], Step [37/49], Loss: 1.2269\n",
      "Epoch [10/50], Step [41/49], Loss: 1.4279\n",
      "Epoch [10/50], Step [45/49], Loss: 1.4329\n",
      "Epoch [10/50], Step [49/49], Loss: 1.1266\n",
      "Validation loss:\t 48.81910945647206\n",
      "Epoch [11/50], Step [1/49], Loss: 1.6434\n",
      "Epoch [11/50], Step [5/49], Loss: 1.3783\n",
      "Epoch [11/50], Step [9/49], Loss: 1.3281\n",
      "Epoch [11/50], Step [13/49], Loss: 1.1821\n",
      "Epoch [11/50], Step [17/49], Loss: 1.0693\n",
      "Epoch [11/50], Step [21/49], Loss: 1.3007\n",
      "Epoch [11/50], Step [25/49], Loss: 1.4217\n",
      "Epoch [11/50], Step [29/49], Loss: 1.1195\n",
      "Epoch [11/50], Step [33/49], Loss: 1.1789\n",
      "Epoch [11/50], Step [37/49], Loss: 1.2666\n",
      "Epoch [11/50], Step [41/49], Loss: 1.5350\n",
      "Epoch [11/50], Step [45/49], Loss: 1.2665\n",
      "Epoch [11/50], Step [49/49], Loss: 1.3540\n",
      "Validation loss:\t 49.803559919320165\n",
      "Epoch [12/50], Step [1/49], Loss: 1.6193\n",
      "Epoch [12/50], Step [5/49], Loss: 1.3211\n",
      "Epoch [12/50], Step [9/49], Loss: 1.1680\n",
      "Epoch [12/50], Step [13/49], Loss: 1.1443\n",
      "Epoch [12/50], Step [17/49], Loss: 1.0496\n",
      "Epoch [12/50], Step [21/49], Loss: 1.1957\n",
      "Epoch [12/50], Step [25/49], Loss: 1.0351\n",
      "Epoch [12/50], Step [29/49], Loss: 1.0906\n",
      "Epoch [12/50], Step [33/49], Loss: 1.1222\n",
      "Epoch [12/50], Step [37/49], Loss: 1.2565\n",
      "Epoch [12/50], Step [41/49], Loss: 1.1524\n",
      "Epoch [12/50], Step [45/49], Loss: 1.2659\n",
      "Epoch [12/50], Step [49/49], Loss: 1.3618\n",
      "Validation loss:\t 45.681738836239006\n",
      "Epoch [13/50], Step [1/49], Loss: 1.1363\n",
      "Epoch [13/50], Step [5/49], Loss: 1.2433\n",
      "Epoch [13/50], Step [9/49], Loss: 1.2565\n",
      "Epoch [13/50], Step [13/49], Loss: 1.1252\n",
      "Epoch [13/50], Step [17/49], Loss: 1.0983\n",
      "Epoch [13/50], Step [21/49], Loss: 1.4151\n",
      "Epoch [13/50], Step [25/49], Loss: 1.0288\n",
      "Epoch [13/50], Step [29/49], Loss: 1.4189\n",
      "Epoch [13/50], Step [33/49], Loss: 0.9828\n",
      "Epoch [13/50], Step [37/49], Loss: 1.2629\n",
      "Epoch [13/50], Step [41/49], Loss: 1.0511\n",
      "Epoch [13/50], Step [45/49], Loss: 1.3303\n",
      "Epoch [13/50], Step [49/49], Loss: 1.4775\n",
      "Validation loss:\t 49.38571409861172\n",
      "Epoch [14/50], Step [1/49], Loss: 1.3142\n",
      "Epoch [14/50], Step [5/49], Loss: 1.0393\n",
      "Epoch [14/50], Step [9/49], Loss: 1.1949\n",
      "Epoch [14/50], Step [13/49], Loss: 1.1414\n",
      "Epoch [14/50], Step [17/49], Loss: 1.2527\n",
      "Epoch [14/50], Step [21/49], Loss: 1.0667\n",
      "Epoch [14/50], Step [25/49], Loss: 1.2322\n",
      "Epoch [14/50], Step [29/49], Loss: 1.1594\n",
      "Epoch [14/50], Step [33/49], Loss: 1.1621\n",
      "Epoch [14/50], Step [37/49], Loss: 1.0105\n",
      "Epoch [14/50], Step [41/49], Loss: 1.4475\n",
      "Epoch [14/50], Step [45/49], Loss: 1.3333\n",
      "Epoch [14/50], Step [49/49], Loss: 1.5367\n",
      "Validation loss:\t 46.54884294646236\n",
      "Epoch [15/50], Step [1/49], Loss: 1.3013\n",
      "Epoch [15/50], Step [5/49], Loss: 1.1422\n",
      "Epoch [15/50], Step [9/49], Loss: 0.8916\n",
      "Epoch [15/50], Step [13/49], Loss: 1.2831\n",
      "Epoch [15/50], Step [17/49], Loss: 1.0627\n",
      "Epoch [15/50], Step [21/49], Loss: 1.2387\n",
      "Epoch [15/50], Step [25/49], Loss: 1.5638\n",
      "Epoch [15/50], Step [29/49], Loss: 1.0871\n",
      "Epoch [15/50], Step [33/49], Loss: 1.0721\n",
      "Epoch [15/50], Step [37/49], Loss: 1.0250\n",
      "Epoch [15/50], Step [41/49], Loss: 1.0456\n",
      "Epoch [15/50], Step [45/49], Loss: 1.1597\n",
      "Epoch [15/50], Step [49/49], Loss: 1.6594\n",
      "Validation loss:\t 48.59574080736079\n",
      "Epoch [16/50], Step [1/49], Loss: 1.0703\n",
      "Epoch [16/50], Step [5/49], Loss: 1.1791\n",
      "Epoch [16/50], Step [9/49], Loss: 1.1378\n",
      "Epoch [16/50], Step [13/49], Loss: 1.1546\n",
      "Epoch [16/50], Step [17/49], Loss: 1.0359\n",
      "Epoch [16/50], Step [21/49], Loss: 1.0849\n",
      "Epoch [16/50], Step [25/49], Loss: 0.9849\n",
      "Epoch [16/50], Step [29/49], Loss: 0.9425\n",
      "Epoch [16/50], Step [33/49], Loss: 1.3411\n",
      "Epoch [16/50], Step [37/49], Loss: 1.2490\n",
      "Epoch [16/50], Step [41/49], Loss: 0.9999\n",
      "Epoch [16/50], Step [45/49], Loss: 1.0266\n",
      "Epoch [16/50], Step [49/49], Loss: 0.9931\n",
      "Validation loss:\t 47.55186814778749\n",
      "Epoch [17/50], Step [1/49], Loss: 1.0301\n",
      "Epoch [17/50], Step [5/49], Loss: 0.8679\n",
      "Epoch [17/50], Step [9/49], Loss: 1.0163\n",
      "Epoch [17/50], Step [13/49], Loss: 1.1008\n",
      "Epoch [17/50], Step [17/49], Loss: 1.0860\n",
      "Epoch [17/50], Step [21/49], Loss: 1.1365\n",
      "Epoch [17/50], Step [25/49], Loss: 1.0127\n",
      "Epoch [17/50], Step [29/49], Loss: 0.9919\n",
      "Epoch [17/50], Step [33/49], Loss: 1.0959\n",
      "Epoch [17/50], Step [37/49], Loss: 1.0379\n",
      "Epoch [17/50], Step [41/49], Loss: 0.9721\n",
      "Epoch [17/50], Step [45/49], Loss: 0.9617\n",
      "Epoch [17/50], Step [49/49], Loss: 1.2373\n",
      "Validation loss:\t 52.73778004227733\n",
      "Epoch [18/50], Step [1/49], Loss: 1.4294\n",
      "Epoch [18/50], Step [5/49], Loss: 0.9769\n",
      "Epoch [18/50], Step [9/49], Loss: 0.9351\n",
      "Epoch [18/50], Step [13/49], Loss: 1.3906\n",
      "Epoch [18/50], Step [17/49], Loss: 0.9348\n",
      "Epoch [18/50], Step [21/49], Loss: 1.0545\n",
      "Epoch [18/50], Step [25/49], Loss: 0.9940\n",
      "Epoch [18/50], Step [29/49], Loss: 1.1958\n",
      "Epoch [18/50], Step [33/49], Loss: 1.2164\n",
      "Epoch [18/50], Step [37/49], Loss: 0.9376\n",
      "Epoch [18/50], Step [41/49], Loss: 1.0497\n",
      "Epoch [18/50], Step [45/49], Loss: 0.9964\n",
      "Epoch [18/50], Step [49/49], Loss: 1.2080\n",
      "Validation loss:\t 47.445914351186126\n",
      "Epoch [19/50], Step [1/49], Loss: 0.7642\n",
      "Epoch [19/50], Step [5/49], Loss: 0.9509\n",
      "Epoch [19/50], Step [9/49], Loss: 0.9096\n",
      "Epoch [19/50], Step [13/49], Loss: 1.1366\n",
      "Epoch [19/50], Step [17/49], Loss: 0.8369\n",
      "Epoch [19/50], Step [21/49], Loss: 0.8406\n",
      "Epoch [19/50], Step [25/49], Loss: 0.9657\n",
      "Epoch [19/50], Step [29/49], Loss: 0.9270\n",
      "Epoch [19/50], Step [33/49], Loss: 0.9979\n",
      "Epoch [19/50], Step [37/49], Loss: 0.9689\n",
      "Epoch [19/50], Step [41/49], Loss: 1.0241\n",
      "Epoch [19/50], Step [45/49], Loss: 1.0168\n",
      "Epoch [19/50], Step [49/49], Loss: 1.4363\n",
      "Validation loss:\t 46.36896517609562\n",
      "Epoch [20/50], Step [1/49], Loss: 0.9350\n",
      "Epoch [20/50], Step [5/49], Loss: 1.1056\n",
      "Epoch [20/50], Step [9/49], Loss: 0.8559\n",
      "Epoch [20/50], Step [13/49], Loss: 1.2096\n",
      "Epoch [20/50], Step [17/49], Loss: 1.1176\n",
      "Epoch [20/50], Step [21/49], Loss: 0.7322\n",
      "Epoch [20/50], Step [25/49], Loss: 0.8915\n",
      "Epoch [20/50], Step [29/49], Loss: 0.9497\n",
      "Epoch [20/50], Step [33/49], Loss: 1.0282\n",
      "Epoch [20/50], Step [37/49], Loss: 0.9949\n",
      "Epoch [20/50], Step [41/49], Loss: 0.8322\n",
      "Epoch [20/50], Step [45/49], Loss: 1.0210\n",
      "Epoch [20/50], Step [49/49], Loss: 0.7375\n",
      "Validation loss:\t 45.71622183440298\n",
      "Epoch [21/50], Step [1/49], Loss: 0.7862\n",
      "Epoch [21/50], Step [5/49], Loss: 0.9157\n",
      "Epoch [21/50], Step [9/49], Loss: 0.7947\n",
      "Epoch [21/50], Step [13/49], Loss: 0.7943\n",
      "Epoch [21/50], Step [17/49], Loss: 0.8156\n",
      "Epoch [21/50], Step [21/49], Loss: 0.8025\n",
      "Epoch [21/50], Step [25/49], Loss: 0.9681\n",
      "Epoch [21/50], Step [29/49], Loss: 0.9708\n",
      "Epoch [21/50], Step [33/49], Loss: 1.0246\n",
      "Epoch [21/50], Step [37/49], Loss: 0.8722\n",
      "Epoch [21/50], Step [41/49], Loss: 0.8850\n",
      "Epoch [21/50], Step [45/49], Loss: 0.6688\n",
      "Epoch [21/50], Step [49/49], Loss: 0.8696\n",
      "Validation loss:\t 46.07634859450776\n",
      "Epoch [22/50], Step [1/49], Loss: 0.9038\n",
      "Epoch [22/50], Step [5/49], Loss: 0.9504\n",
      "Epoch [22/50], Step [9/49], Loss: 0.8873\n",
      "Epoch [22/50], Step [13/49], Loss: 0.9146\n",
      "Epoch [22/50], Step [17/49], Loss: 0.8481\n",
      "Epoch [22/50], Step [21/49], Loss: 0.7386\n",
      "Epoch [22/50], Step [25/49], Loss: 1.2057\n",
      "Epoch [22/50], Step [29/49], Loss: 0.8141\n",
      "Epoch [22/50], Step [33/49], Loss: 0.7195\n",
      "Epoch [22/50], Step [37/49], Loss: 1.2008\n",
      "Epoch [22/50], Step [41/49], Loss: 0.8453\n",
      "Epoch [22/50], Step [45/49], Loss: 1.0830\n",
      "Epoch [22/50], Step [49/49], Loss: 1.3930\n",
      "Validation loss:\t 45.53840983283001\n",
      "Epoch [23/50], Step [1/49], Loss: 1.0181\n",
      "Epoch [23/50], Step [5/49], Loss: 0.8653\n",
      "Epoch [23/50], Step [9/49], Loss: 0.7297\n",
      "Epoch [23/50], Step [13/49], Loss: 0.9109\n",
      "Epoch [23/50], Step [17/49], Loss: 0.8265\n",
      "Epoch [23/50], Step [21/49], Loss: 0.7202\n",
      "Epoch [23/50], Step [25/49], Loss: 0.9174\n",
      "Epoch [23/50], Step [29/49], Loss: 0.9919\n",
      "Epoch [23/50], Step [33/49], Loss: 0.6958\n",
      "Epoch [23/50], Step [37/49], Loss: 0.8341\n",
      "Epoch [23/50], Step [41/49], Loss: 0.9024\n",
      "Epoch [23/50], Step [45/49], Loss: 0.8157\n",
      "Epoch [23/50], Step [49/49], Loss: 0.9607\n",
      "Validation loss:\t 47.12442131481386\n",
      "Epoch [24/50], Step [1/49], Loss: 1.0304\n",
      "Epoch [24/50], Step [5/49], Loss: 0.7607\n",
      "Epoch [24/50], Step [9/49], Loss: 0.7463\n",
      "Epoch [24/50], Step [13/49], Loss: 0.7067\n",
      "Epoch [24/50], Step [17/49], Loss: 0.6553\n",
      "Epoch [24/50], Step [21/49], Loss: 0.7316\n",
      "Epoch [24/50], Step [25/49], Loss: 0.7589\n",
      "Epoch [24/50], Step [29/49], Loss: 0.7066\n",
      "Epoch [24/50], Step [33/49], Loss: 0.9292\n",
      "Epoch [24/50], Step [37/49], Loss: 0.9745\n",
      "Epoch [24/50], Step [41/49], Loss: 0.7139\n",
      "Epoch [24/50], Step [45/49], Loss: 1.0336\n",
      "Epoch [24/50], Step [49/49], Loss: 0.8725\n",
      "Validation loss:\t 45.9715548368301\n",
      "Epoch [25/50], Step [1/49], Loss: 0.8566\n",
      "Epoch [25/50], Step [5/49], Loss: 0.6784\n",
      "Epoch [25/50], Step [9/49], Loss: 0.7707\n",
      "Epoch [25/50], Step [13/49], Loss: 0.8179\n",
      "Epoch [25/50], Step [17/49], Loss: 0.8093\n",
      "Epoch [25/50], Step [21/49], Loss: 0.8343\n",
      "Epoch [25/50], Step [25/49], Loss: 0.9412\n",
      "Epoch [25/50], Step [29/49], Loss: 0.6919\n",
      "Epoch [25/50], Step [33/49], Loss: 0.9146\n",
      "Epoch [25/50], Step [37/49], Loss: 0.8902\n",
      "Epoch [25/50], Step [41/49], Loss: 0.7658\n",
      "Epoch [25/50], Step [45/49], Loss: 0.8779\n",
      "Epoch [25/50], Step [49/49], Loss: 0.8481\n",
      "Validation loss:\t 46.63834176268729\n",
      "Epoch [26/50], Step [1/49], Loss: 0.8511\n",
      "Epoch [26/50], Step [5/49], Loss: 0.6901\n",
      "Epoch [26/50], Step [9/49], Loss: 0.6651\n",
      "Epoch [26/50], Step [13/49], Loss: 0.9294\n",
      "Epoch [26/50], Step [17/49], Loss: 0.8668\n",
      "Epoch [26/50], Step [21/49], Loss: 0.6540\n",
      "Epoch [26/50], Step [25/49], Loss: 0.8724\n",
      "Epoch [26/50], Step [29/49], Loss: 0.8314\n",
      "Epoch [26/50], Step [33/49], Loss: 0.5793\n",
      "Epoch [26/50], Step [37/49], Loss: 0.7585\n",
      "Epoch [26/50], Step [41/49], Loss: 0.8996\n",
      "Epoch [26/50], Step [45/49], Loss: 0.8636\n",
      "Epoch [26/50], Step [49/49], Loss: 1.3356\n",
      "Validation loss:\t 45.651351819295\n",
      "Epoch [27/50], Step [1/49], Loss: 0.8710\n",
      "Epoch [27/50], Step [5/49], Loss: 0.9132\n",
      "Epoch [27/50], Step [9/49], Loss: 0.7959\n",
      "Epoch [27/50], Step [13/49], Loss: 0.6718\n",
      "Epoch [27/50], Step [17/49], Loss: 0.8850\n",
      "Epoch [27/50], Step [21/49], Loss: 1.0192\n",
      "Epoch [27/50], Step [25/49], Loss: 0.8040\n",
      "Epoch [27/50], Step [29/49], Loss: 0.6707\n",
      "Epoch [27/50], Step [33/49], Loss: 0.7186\n",
      "Epoch [27/50], Step [37/49], Loss: 0.5888\n",
      "Epoch [27/50], Step [41/49], Loss: 0.7370\n",
      "Epoch [27/50], Step [45/49], Loss: 0.7749\n",
      "Epoch [27/50], Step [49/49], Loss: 0.7133\n",
      "Validation loss:\t 46.05763859405535\n",
      "Epoch [28/50], Step [1/49], Loss: 0.6504\n",
      "Epoch [28/50], Step [5/49], Loss: 0.7173\n",
      "Epoch [28/50], Step [9/49], Loss: 0.6955\n",
      "Epoch [28/50], Step [13/49], Loss: 0.8531\n",
      "Epoch [28/50], Step [17/49], Loss: 0.6417\n",
      "Epoch [28/50], Step [21/49], Loss: 0.7122\n",
      "Epoch [28/50], Step [25/49], Loss: 0.7294\n",
      "Epoch [28/50], Step [29/49], Loss: 0.7744\n",
      "Epoch [28/50], Step [33/49], Loss: 0.5523\n",
      "Epoch [28/50], Step [37/49], Loss: 0.7755\n",
      "Epoch [28/50], Step [41/49], Loss: 0.6745\n",
      "Epoch [28/50], Step [45/49], Loss: 0.7015\n",
      "Epoch [28/50], Step [49/49], Loss: 0.7288\n",
      "Validation loss:\t 45.22868899211333\n",
      "Epoch [29/50], Step [1/49], Loss: 0.6499\n",
      "Epoch [29/50], Step [5/49], Loss: 0.6789\n",
      "Epoch [29/50], Step [9/49], Loss: 0.5738\n",
      "Epoch [29/50], Step [13/49], Loss: 0.7216\n",
      "Epoch [29/50], Step [17/49], Loss: 0.8270\n",
      "Epoch [29/50], Step [21/49], Loss: 0.6834\n",
      "Epoch [29/50], Step [25/49], Loss: 0.8951\n",
      "Epoch [29/50], Step [29/49], Loss: 0.9136\n",
      "Epoch [29/50], Step [33/49], Loss: 0.6330\n",
      "Epoch [29/50], Step [37/49], Loss: 0.7469\n",
      "Epoch [29/50], Step [41/49], Loss: 0.7055\n",
      "Epoch [29/50], Step [45/49], Loss: 0.6225\n",
      "Epoch [29/50], Step [49/49], Loss: 0.7515\n",
      "Validation loss:\t 46.27851393450161\n",
      "Epoch [30/50], Step [1/49], Loss: 0.6803\n",
      "Epoch [30/50], Step [5/49], Loss: 0.7250\n",
      "Epoch [30/50], Step [9/49], Loss: 0.7499\n",
      "Epoch [30/50], Step [13/49], Loss: 0.5561\n",
      "Epoch [30/50], Step [17/49], Loss: 0.7845\n",
      "Epoch [30/50], Step [21/49], Loss: 0.6597\n",
      "Epoch [30/50], Step [25/49], Loss: 0.6404\n",
      "Epoch [30/50], Step [29/49], Loss: 0.6642\n",
      "Epoch [30/50], Step [33/49], Loss: 0.8679\n",
      "Epoch [30/50], Step [37/49], Loss: 0.7507\n",
      "Epoch [30/50], Step [41/49], Loss: 0.7026\n",
      "Epoch [30/50], Step [45/49], Loss: 0.6437\n",
      "Epoch [30/50], Step [49/49], Loss: 0.9184\n",
      "Validation loss:\t 45.697973041965064\n",
      "Epoch [31/50], Step [1/49], Loss: 0.7097\n",
      "Epoch [31/50], Step [5/49], Loss: 0.6972\n",
      "Epoch [31/50], Step [9/49], Loss: 0.5994\n",
      "Epoch [31/50], Step [13/49], Loss: 0.7157\n",
      "Epoch [31/50], Step [17/49], Loss: 0.6184\n",
      "Epoch [31/50], Step [21/49], Loss: 0.6977\n",
      "Epoch [31/50], Step [25/49], Loss: 0.7002\n",
      "Epoch [31/50], Step [29/49], Loss: 0.7968\n",
      "Epoch [31/50], Step [33/49], Loss: 0.6320\n",
      "Epoch [31/50], Step [37/49], Loss: 0.8218\n",
      "Epoch [31/50], Step [41/49], Loss: 0.5666\n",
      "Epoch [31/50], Step [45/49], Loss: 0.6925\n",
      "Epoch [31/50], Step [49/49], Loss: 0.7482\n",
      "Validation loss:\t 46.5368598680045\n",
      "Epoch [32/50], Step [1/49], Loss: 0.6260\n",
      "Epoch [32/50], Step [5/49], Loss: 0.6577\n",
      "Epoch [32/50], Step [9/49], Loss: 0.5902\n",
      "Epoch [32/50], Step [13/49], Loss: 0.5290\n",
      "Epoch [32/50], Step [17/49], Loss: 0.6273\n",
      "Epoch [32/50], Step [21/49], Loss: 0.7087\n",
      "Epoch [32/50], Step [25/49], Loss: 0.7244\n",
      "Epoch [32/50], Step [29/49], Loss: 0.6596\n",
      "Epoch [32/50], Step [33/49], Loss: 0.5204\n",
      "Epoch [32/50], Step [37/49], Loss: 0.6445\n",
      "Epoch [32/50], Step [41/49], Loss: 0.6969\n",
      "Epoch [32/50], Step [45/49], Loss: 0.7513\n",
      "Epoch [32/50], Step [49/49], Loss: 0.6759\n",
      "Validation loss:\t 46.138980349924985\n",
      "Epoch [33/50], Step [1/49], Loss: 0.6489\n",
      "Epoch [33/50], Step [5/49], Loss: 0.5796\n",
      "Epoch [33/50], Step [9/49], Loss: 0.6639\n",
      "Epoch [33/50], Step [13/49], Loss: 0.6408\n",
      "Epoch [33/50], Step [17/49], Loss: 0.6841\n",
      "Epoch [33/50], Step [21/49], Loss: 0.5287\n",
      "Epoch [33/50], Step [25/49], Loss: 0.8710\n",
      "Epoch [33/50], Step [29/49], Loss: 0.5762\n",
      "Epoch [33/50], Step [33/49], Loss: 0.5552\n",
      "Epoch [33/50], Step [37/49], Loss: 0.5210\n",
      "Epoch [33/50], Step [41/49], Loss: 0.5492\n",
      "Epoch [33/50], Step [45/49], Loss: 0.7593\n",
      "Epoch [33/50], Step [49/49], Loss: 0.7194\n",
      "Validation loss:\t 46.471245872842736\n",
      "Epoch [34/50], Step [1/49], Loss: 0.8681\n",
      "Epoch [34/50], Step [5/49], Loss: 0.6223\n",
      "Epoch [34/50], Step [9/49], Loss: 0.5679\n",
      "Epoch [34/50], Step [13/49], Loss: 0.6024\n",
      "Epoch [34/50], Step [17/49], Loss: 0.6964\n",
      "Epoch [34/50], Step [21/49], Loss: 0.5813\n",
      "Epoch [34/50], Step [25/49], Loss: 0.5084\n",
      "Epoch [34/50], Step [29/49], Loss: 0.6422\n",
      "Epoch [34/50], Step [33/49], Loss: 0.5161\n",
      "Epoch [34/50], Step [37/49], Loss: 0.6660\n",
      "Epoch [34/50], Step [41/49], Loss: 0.7716\n",
      "Epoch [34/50], Step [45/49], Loss: 0.5602\n",
      "Epoch [34/50], Step [49/49], Loss: 0.6594\n",
      "Validation loss:\t 46.10790215440528\n",
      "Epoch [35/50], Step [1/49], Loss: 0.7282\n",
      "Epoch [35/50], Step [5/49], Loss: 0.6063\n",
      "Epoch [35/50], Step [9/49], Loss: 0.6805\n",
      "Epoch [35/50], Step [13/49], Loss: 0.6663\n",
      "Epoch [35/50], Step [17/49], Loss: 0.6913\n",
      "Epoch [35/50], Step [21/49], Loss: 0.5706\n",
      "Epoch [35/50], Step [25/49], Loss: 0.7243\n",
      "Epoch [35/50], Step [29/49], Loss: 0.6787\n",
      "Epoch [35/50], Step [33/49], Loss: 0.6302\n",
      "Epoch [35/50], Step [37/49], Loss: 0.5809\n",
      "Epoch [35/50], Step [41/49], Loss: 0.6180\n",
      "Epoch [35/50], Step [45/49], Loss: 0.7068\n",
      "Epoch [35/50], Step [49/49], Loss: 0.4022\n",
      "Validation loss:\t 45.016867730380426\n",
      "Epoch [36/50], Step [1/49], Loss: 0.7027\n",
      "Epoch [36/50], Step [5/49], Loss: 0.6946\n",
      "Epoch [36/50], Step [9/49], Loss: 0.5072\n",
      "Epoch [36/50], Step [13/49], Loss: 0.5629\n",
      "Epoch [36/50], Step [17/49], Loss: 0.6368\n",
      "Epoch [36/50], Step [21/49], Loss: 0.7047\n",
      "Epoch [36/50], Step [25/49], Loss: 0.6039\n",
      "Epoch [36/50], Step [29/49], Loss: 0.5165\n",
      "Epoch [36/50], Step [33/49], Loss: 0.6206\n",
      "Epoch [36/50], Step [37/49], Loss: 0.5695\n",
      "Epoch [36/50], Step [41/49], Loss: 0.7895\n",
      "Epoch [36/50], Step [45/49], Loss: 0.6533\n",
      "Epoch [36/50], Step [49/49], Loss: 0.5038\n",
      "Validation loss:\t 45.5579894971729\n",
      "Epoch [37/50], Step [1/49], Loss: 0.5944\n",
      "Epoch [37/50], Step [5/49], Loss: 0.5740\n",
      "Epoch [37/50], Step [9/49], Loss: 0.6918\n",
      "Epoch [37/50], Step [13/49], Loss: 0.5716\n",
      "Epoch [37/50], Step [17/49], Loss: 0.6039\n",
      "Epoch [37/50], Step [21/49], Loss: 0.5674\n",
      "Epoch [37/50], Step [25/49], Loss: 0.6000\n",
      "Epoch [37/50], Step [29/49], Loss: 0.4521\n",
      "Epoch [37/50], Step [33/49], Loss: 0.5286\n",
      "Epoch [37/50], Step [37/49], Loss: 0.6845\n",
      "Epoch [37/50], Step [41/49], Loss: 0.6263\n",
      "Epoch [37/50], Step [45/49], Loss: 0.5981\n",
      "Epoch [37/50], Step [49/49], Loss: 0.5371\n",
      "Validation loss:\t 44.950255498275816\n",
      "Epoch [38/50], Step [1/49], Loss: 0.7008\n",
      "Epoch [38/50], Step [5/49], Loss: 0.5023\n",
      "Epoch [38/50], Step [9/49], Loss: 0.6649\n",
      "Epoch [38/50], Step [13/49], Loss: 0.6456\n",
      "Epoch [38/50], Step [17/49], Loss: 0.6121\n",
      "Epoch [38/50], Step [21/49], Loss: 0.5114\n",
      "Epoch [38/50], Step [25/49], Loss: 0.5258\n",
      "Epoch [38/50], Step [29/49], Loss: 0.6853\n",
      "Epoch [38/50], Step [33/49], Loss: 0.4550\n",
      "Epoch [38/50], Step [37/49], Loss: 0.4893\n",
      "Epoch [38/50], Step [41/49], Loss: 0.5274\n",
      "Epoch [38/50], Step [45/49], Loss: 0.5610\n",
      "Epoch [38/50], Step [49/49], Loss: 0.3763\n",
      "Validation loss:\t 45.95238756609432\n",
      "Epoch [39/50], Step [1/49], Loss: 0.7738\n",
      "Epoch [39/50], Step [5/49], Loss: 0.5044\n",
      "Epoch [39/50], Step [9/49], Loss: 0.4871\n",
      "Epoch [39/50], Step [13/49], Loss: 0.5035\n",
      "Epoch [39/50], Step [17/49], Loss: 0.5887\n",
      "Epoch [39/50], Step [21/49], Loss: 0.5545\n",
      "Epoch [39/50], Step [25/49], Loss: 0.4842\n",
      "Epoch [39/50], Step [29/49], Loss: 0.4401\n",
      "Epoch [39/50], Step [33/49], Loss: 0.6048\n",
      "Epoch [39/50], Step [37/49], Loss: 0.8041\n",
      "Epoch [39/50], Step [41/49], Loss: 0.4817\n",
      "Epoch [39/50], Step [45/49], Loss: 0.5812\n",
      "Epoch [39/50], Step [49/49], Loss: 0.8834\n",
      "Validation loss:\t 46.239948762563586\n",
      "Epoch [40/50], Step [1/49], Loss: 0.5853\n",
      "Epoch [40/50], Step [5/49], Loss: 0.8202\n",
      "Epoch [40/50], Step [9/49], Loss: 0.5845\n",
      "Epoch [40/50], Step [13/49], Loss: 0.5706\n",
      "Epoch [40/50], Step [17/49], Loss: 0.4435\n",
      "Epoch [40/50], Step [21/49], Loss: 0.5580\n",
      "Epoch [40/50], Step [25/49], Loss: 0.4163\n",
      "Epoch [40/50], Step [29/49], Loss: 0.5580\n",
      "Epoch [40/50], Step [33/49], Loss: 0.5909\n",
      "Epoch [40/50], Step [37/49], Loss: 0.5611\n",
      "Epoch [40/50], Step [41/49], Loss: 0.6292\n",
      "Epoch [40/50], Step [45/49], Loss: 0.5679\n",
      "Epoch [40/50], Step [49/49], Loss: 0.7150\n",
      "Validation loss:\t 45.73328277841468\n",
      "Epoch [41/50], Step [1/49], Loss: 0.3635\n",
      "Epoch [41/50], Step [5/49], Loss: 0.4950\n",
      "Epoch [41/50], Step [9/49], Loss: 0.5554\n",
      "Epoch [41/50], Step [13/49], Loss: 0.5403\n",
      "Epoch [41/50], Step [17/49], Loss: 0.3919\n",
      "Epoch [41/50], Step [21/49], Loss: 0.6289\n",
      "Epoch [41/50], Step [25/49], Loss: 0.5437\n",
      "Epoch [41/50], Step [29/49], Loss: 0.4455\n",
      "Epoch [41/50], Step [33/49], Loss: 0.5179\n",
      "Epoch [41/50], Step [37/49], Loss: 0.5356\n",
      "Epoch [41/50], Step [41/49], Loss: 0.4370\n",
      "Epoch [41/50], Step [45/49], Loss: 0.4554\n",
      "Epoch [41/50], Step [49/49], Loss: 0.5265\n",
      "Validation loss:\t 45.01565406864409\n",
      "Epoch [42/50], Step [1/49], Loss: 0.5345\n",
      "Epoch [42/50], Step [5/49], Loss: 0.5482\n",
      "Epoch [42/50], Step [9/49], Loss: 0.6486\n",
      "Epoch [42/50], Step [13/49], Loss: 0.4177\n",
      "Epoch [42/50], Step [17/49], Loss: 0.5694\n",
      "Epoch [42/50], Step [21/49], Loss: 0.4777\n",
      "Epoch [42/50], Step [25/49], Loss: 0.5576\n",
      "Epoch [42/50], Step [29/49], Loss: 0.4962\n",
      "Epoch [42/50], Step [33/49], Loss: 0.5662\n",
      "Epoch [42/50], Step [37/49], Loss: 0.5192\n",
      "Epoch [42/50], Step [41/49], Loss: 0.4641\n",
      "Epoch [42/50], Step [45/49], Loss: 0.5415\n",
      "Epoch [42/50], Step [49/49], Loss: 0.6038\n",
      "Validation loss:\t 44.336432298755625\n",
      "Epoch [43/50], Step [1/49], Loss: 0.4631\n",
      "Epoch [43/50], Step [5/49], Loss: 0.5281\n",
      "Epoch [43/50], Step [9/49], Loss: 0.5708\n",
      "Epoch [43/50], Step [13/49], Loss: 0.5914\n",
      "Epoch [43/50], Step [17/49], Loss: 0.4101\n",
      "Epoch [43/50], Step [21/49], Loss: 0.4293\n",
      "Epoch [43/50], Step [25/49], Loss: 0.4067\n",
      "Epoch [43/50], Step [29/49], Loss: 0.5081\n",
      "Epoch [43/50], Step [33/49], Loss: 0.5822\n",
      "Epoch [43/50], Step [37/49], Loss: 0.5478\n",
      "Epoch [43/50], Step [41/49], Loss: 0.6070\n",
      "Epoch [43/50], Step [45/49], Loss: 0.5683\n",
      "Epoch [43/50], Step [49/49], Loss: 0.2822\n",
      "Validation loss:\t 45.742252876493666\n",
      "Epoch [44/50], Step [1/49], Loss: 0.5704\n",
      "Epoch [44/50], Step [5/49], Loss: 0.5650\n",
      "Epoch [44/50], Step [9/49], Loss: 0.5164\n",
      "Epoch [44/50], Step [13/49], Loss: 0.4695\n",
      "Epoch [44/50], Step [17/49], Loss: 0.4171\n",
      "Epoch [44/50], Step [21/49], Loss: 0.4915\n",
      "Epoch [44/50], Step [25/49], Loss: 0.5261\n",
      "Epoch [44/50], Step [29/49], Loss: 0.3503\n",
      "Epoch [44/50], Step [33/49], Loss: 0.3780\n",
      "Epoch [44/50], Step [37/49], Loss: 0.4605\n",
      "Epoch [44/50], Step [41/49], Loss: 0.4459\n",
      "Epoch [44/50], Step [45/49], Loss: 0.4931\n",
      "Epoch [44/50], Step [49/49], Loss: 0.6320\n",
      "Validation loss:\t 46.95374236371303\n",
      "Epoch [45/50], Step [1/49], Loss: 0.4450\n",
      "Epoch [45/50], Step [5/49], Loss: 0.4243\n",
      "Epoch [45/50], Step [9/49], Loss: 0.4519\n",
      "Epoch [45/50], Step [13/49], Loss: 0.4886\n",
      "Epoch [45/50], Step [17/49], Loss: 0.5560\n",
      "Epoch [45/50], Step [21/49], Loss: 0.5780\n",
      "Epoch [45/50], Step [25/49], Loss: 0.4425\n",
      "Epoch [45/50], Step [29/49], Loss: 0.4763\n",
      "Epoch [45/50], Step [33/49], Loss: 0.5783\n",
      "Epoch [45/50], Step [37/49], Loss: 0.5628\n",
      "Epoch [45/50], Step [41/49], Loss: 0.5505\n",
      "Epoch [45/50], Step [45/49], Loss: 0.4416\n",
      "Epoch [45/50], Step [49/49], Loss: 0.5908\n",
      "Validation loss:\t 45.73649089976228\n",
      "Epoch [46/50], Step [1/49], Loss: 0.4544\n",
      "Epoch [46/50], Step [5/49], Loss: 0.4296\n",
      "Epoch [46/50], Step [9/49], Loss: 0.4760\n",
      "Epoch [46/50], Step [13/49], Loss: 0.3335\n",
      "Epoch [46/50], Step [17/49], Loss: 0.4789\n",
      "Epoch [46/50], Step [21/49], Loss: 0.6225\n",
      "Epoch [46/50], Step [25/49], Loss: 0.4507\n",
      "Epoch [46/50], Step [29/49], Loss: 0.5173\n",
      "Epoch [46/50], Step [33/49], Loss: 0.4946\n",
      "Epoch [46/50], Step [37/49], Loss: 0.5463\n",
      "Epoch [46/50], Step [41/49], Loss: 0.6886\n",
      "Epoch [46/50], Step [45/49], Loss: 0.3368\n",
      "Epoch [46/50], Step [49/49], Loss: 0.5285\n",
      "Validation loss:\t 45.307602459475305\n",
      "Epoch [47/50], Step [1/49], Loss: 0.3984\n",
      "Epoch [47/50], Step [5/49], Loss: 0.4126\n",
      "Epoch [47/50], Step [9/49], Loss: 0.4482\n",
      "Epoch [47/50], Step [13/49], Loss: 0.4758\n",
      "Epoch [47/50], Step [17/49], Loss: 0.3678\n",
      "Epoch [47/50], Step [21/49], Loss: 0.4012\n",
      "Epoch [47/50], Step [25/49], Loss: 0.3832\n",
      "Epoch [47/50], Step [29/49], Loss: 0.4017\n",
      "Epoch [47/50], Step [33/49], Loss: 0.4207\n",
      "Epoch [47/50], Step [37/49], Loss: 0.4905\n",
      "Epoch [47/50], Step [41/49], Loss: 0.5824\n",
      "Epoch [47/50], Step [45/49], Loss: 0.5092\n",
      "Epoch [47/50], Step [49/49], Loss: 0.3754\n",
      "Validation loss:\t 46.41070248081829\n",
      "Epoch [48/50], Step [1/49], Loss: 0.5427\n",
      "Epoch [48/50], Step [5/49], Loss: 0.4546\n",
      "Epoch [48/50], Step [9/49], Loss: 0.4278\n",
      "Epoch [48/50], Step [13/49], Loss: 0.5050\n",
      "Epoch [48/50], Step [17/49], Loss: 0.4135\n",
      "Epoch [48/50], Step [21/49], Loss: 0.4357\n",
      "Epoch [48/50], Step [25/49], Loss: 0.4040\n",
      "Epoch [48/50], Step [29/49], Loss: 0.3753\n",
      "Epoch [48/50], Step [33/49], Loss: 0.4651\n",
      "Epoch [48/50], Step [37/49], Loss: 0.4987\n",
      "Epoch [48/50], Step [41/49], Loss: 0.4865\n",
      "Epoch [48/50], Step [45/49], Loss: 0.3823\n",
      "Epoch [48/50], Step [49/49], Loss: 0.7615\n",
      "Validation loss:\t 46.65566848184743\n",
      "Epoch [49/50], Step [1/49], Loss: 0.5228\n",
      "Epoch [49/50], Step [5/49], Loss: 0.3864\n",
      "Epoch [49/50], Step [9/49], Loss: 0.4240\n",
      "Epoch [49/50], Step [13/49], Loss: 0.3130\n",
      "Epoch [49/50], Step [17/49], Loss: 0.3965\n",
      "Epoch [49/50], Step [21/49], Loss: 0.2890\n",
      "Epoch [49/50], Step [25/49], Loss: 0.4482\n",
      "Epoch [49/50], Step [29/49], Loss: 0.4501\n",
      "Epoch [49/50], Step [33/49], Loss: 0.4182\n",
      "Epoch [49/50], Step [37/49], Loss: 0.2966\n",
      "Epoch [49/50], Step [41/49], Loss: 0.4038\n",
      "Epoch [49/50], Step [45/49], Loss: 0.4542\n",
      "Epoch [49/50], Step [49/49], Loss: 0.6573\n",
      "Validation loss:\t 46.143461562639736\n",
      "Epoch [50/50], Step [1/49], Loss: 0.3650\n",
      "Epoch [50/50], Step [5/49], Loss: 0.5189\n",
      "Epoch [50/50], Step [9/49], Loss: 0.3951\n",
      "Epoch [50/50], Step [13/49], Loss: 0.3215\n",
      "Epoch [50/50], Step [17/49], Loss: 0.3267\n",
      "Epoch [50/50], Step [21/49], Loss: 0.3911\n",
      "Epoch [50/50], Step [25/49], Loss: 0.4386\n",
      "Epoch [50/50], Step [29/49], Loss: 0.4662\n",
      "Epoch [50/50], Step [33/49], Loss: 0.4163\n",
      "Epoch [50/50], Step [37/49], Loss: 0.5480\n",
      "Epoch [50/50], Step [41/49], Loss: 0.4925\n",
      "Epoch [50/50], Step [45/49], Loss: 0.4207\n",
      "Epoch [50/50], Step [49/49], Loss: 0.6404\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "maxseqlen = train_CNN(train_loader_mel, val_loader_mel, \"cnn_single_mel\", mels=128, dataset=mel_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.03      0.04        40\n",
      "           1       0.51      0.47      0.49        40\n",
      "           2       0.35      0.57      0.44        80\n",
      "           3       0.38      0.59      0.46        80\n",
      "           4       0.38      0.38      0.38        40\n",
      "           5       0.16      0.15      0.16        40\n",
      "           6       0.48      0.41      0.44        78\n",
      "           7       0.00      0.00      0.00        40\n",
      "           8       0.37      0.36      0.37       103\n",
      "           9       0.31      0.15      0.20        34\n",
      "\n",
      "    accuracy                           0.36       575\n",
      "   macro avg       0.30      0.31      0.30       575\n",
      "weighted avg       0.33      0.36      0.34       575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_CNN(test_loader_mel, \"cnn_single_mel\", mels=128, maxseqlen=maxseqlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "# class CNN(nn.Module):\n",
    "\n",
    "#     def __init__(self, maxseqlen):\n",
    "#         '''\n",
    "#             maxseqlen: int, dataset.max_length of padding\n",
    "#         '''\n",
    "#         super().__init__()\n",
    "                 \n",
    "#         self.conv = nn.Conv2d(1, 16, 3)           \n",
    "#         self.norm = nn.BatchNorm2d(16)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.maxpool = nn.MaxPool2d(2) \n",
    "#         self.fc = nn.Linear(16*645*63, 10)\n",
    "#         self.dropout = nn.Dropout2d(p=0.28)\n",
    "\n",
    "        \n",
    "#     def forward(self, x):\n",
    "\n",
    "#         x1 = self.conv(x)      # convolutional layer\n",
    "#         x2 = self.norm(x1)     # batch normalization\n",
    "#         x3 = self.relu(x2)     # relu activation \n",
    "#         x4 = self.maxpool(x3)  # max pooling\n",
    "#         [batchsize,a,b,c]= list(x4.size())\n",
    "#         x4 = x4.view(batchsize, a*b*c)\n",
    "#         x5 = self.dropout(x4)  # dropout\n",
    "#         x6 = self.fc(x5)       # fully-connected\n",
    "#         return x6              # softmax (cross-entropy)\n",
    "\n",
    "\n",
    "# def train_CNN(train_loader, val_loader, val_name, mels, dataset):\n",
    "#     '''\n",
    "#         train_loader:  Dataloader\n",
    "#         val_loader:    Dataloader\n",
    "#         val_name:      String\n",
    "#         mels:          int, input size\n",
    "#         dataset:       SpectrumDataset\n",
    "#     '''\n",
    "    \n",
    "#     val_name += \".pkl\"\n",
    "    \n",
    "#     # Device configuration\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#     # Hyper-parameters\n",
    "#     num_classes = 10\n",
    "#     num_epochs = 30\n",
    "#     learning_rate = 0.001\n",
    "#     momentum = 0\n",
    "    \n",
    "#     maxseqlen = dataset.max_length\n",
    "\n",
    "#     model = CNN(maxseqlen).to(device)\n",
    "#     model = model.double()\n",
    "\n",
    "#     # Loss and optimizer\n",
    "#     criterion = nn.CrossEntropyLoss() \n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "#     # Train the model\n",
    "#     train_loss = []\n",
    "#     vl = []\n",
    "#     tl = []\n",
    "#     min_val_loss = np.Inf\n",
    "#     n_epochs_stop = 10000000\n",
    "#     epochs_no_improve = 0\n",
    "#     best_val_loss = np.Inf\n",
    "#     total_step = len(train_loader)\n",
    "#     for epoch in range(num_epochs):\n",
    "#         train_loss.append([])\n",
    "#         val_loss = 0\n",
    "#         early_stop = False\n",
    "\n",
    "\n",
    "#         for i, (feats, labels, lengths) in enumerate(val_loader):\n",
    "#             # Validation set\n",
    "#             feats_reshape = feats.reshape(-1, 1, maxseqlen, mels).to(device)\n",
    "#             labels = labels.to(device) \n",
    "#             labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "#             lengths = lengths.to(device)\n",
    "\n",
    "#             # Forward pass\n",
    "#             outputs = model.forward(feats_reshape)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             val_loss += loss.item()\n",
    "\n",
    "#         vl.append(val_loss / i)\n",
    "#         print(\"Validation loss:\\t\", val_loss)\n",
    "        \n",
    "#         if val_loss <= best_val_loss:\n",
    "#             # Keep the model with minimum validation loss\n",
    "#             best_val_loss = val_loss\n",
    "#             joblib_file = val_name  \n",
    "#             joblib.dump(model, joblib_file)\n",
    "\n",
    "#         v_loss = 0\n",
    "#         for i, (feats, labels, lengths) in enumerate(train_loader):\n",
    "#             # Training set\n",
    "#             feats_reshape = feats.reshape(-1, 1, maxseqlen, mels).to(device)\n",
    "#             labels = labels.to(device) \n",
    "#             labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "#             lengths = lengths.to(device)\n",
    "\n",
    "#             # Forward pass\n",
    "#             outputs = model.forward(feats_reshape)\n",
    "#             loss = criterion(outputs, labels)\n",
    "\n",
    "#             # Backward and optimize\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             if (i) % 4 == 0:\n",
    "#                 print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "#                        .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "#             train_loss[epoch].append(loss.item())\n",
    "\n",
    "#             # epoch loss\n",
    "#             v_loss += loss\n",
    "#             v_loss = v_loss / len(train_loader)\n",
    "\n",
    "#             # If the validation loss is at a minimum\n",
    "#             if v_loss < min_val_loss:\n",
    "\n",
    "#                 #torch.save(model)\n",
    "#                 epochs_no_improve = 0\n",
    "#                 min_val_loss = v_loss\n",
    "\n",
    "#             else:\n",
    "#                 epochs_no_improve += 1\n",
    "#             # Check early stopping condition           \n",
    "#             if epochs_no_improve == n_epochs_stop:\n",
    "#                 print('Early stopping!' )\n",
    "#                 early_stop = True\n",
    "#                 break\n",
    "#             else:\n",
    "#                 continue\n",
    "#             break\n",
    "#         if early_stop:\n",
    "#             print(\"Stopped\")\n",
    "#             break\n",
    "            \n",
    "#         tl.append(np.mean(train_loss[epoch]))\n",
    "\n",
    "#     # Tranform losses to numpy arrays\n",
    "#     tl=np.array(tl)\n",
    "#     vl=np.array(vl)\n",
    "        \n",
    "#     # Plotting learning curve\n",
    "#     plt.figure()\n",
    "#     plt.plot(tl, label=\"training loss\")\n",
    "#     plt.plot(vl, label=\"validation loss\")\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.ylabel('mean loss in the epoch')\n",
    "#     plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "#     plt.show()\n",
    "    \n",
    "#     return maxseqlen\n",
    "    \n",
    "            \n",
    "# import warnings\n",
    "# warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# from sklearn.metrics import classification_report\n",
    "# def score_CNN(test_loader, file_model, mels, maxseqlen):\n",
    "#     # Estimating the model with classification report\n",
    "#     '''\n",
    "#         test_loader:   Dataloader\n",
    "#         file_model:    String, filepath of best model\n",
    "#         mels:          int, input size\n",
    "#         maxseqlen:     int\n",
    "#     '''\n",
    "        \n",
    "#     file_model += \".pkl\"\n",
    "#     model = joblib.load(file_model)\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "#     # Disable batch normalization and dropout in testing\n",
    "#     model.eval()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         y_pred = []\n",
    "#         y_true = []\n",
    "#         for feats, labels, lengths in test_loader:\n",
    "#             feats = feats.reshape(-1, 1, maxseqlen, mels).to(device)\n",
    "#             labels = labels.to(device)\n",
    "#             labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "#             outputs = model.forward(feats).double()\n",
    "#             _, predicted = torch.max(outputs.data, -1)\n",
    "#             y_pred.append(predicted.item())\n",
    "#             y_true.append(labels.item())\n",
    "\n",
    "#         print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
